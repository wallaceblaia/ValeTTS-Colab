# Configuração de Treinamento VITS2 - Dataset Unificado
# Dataset: ~23K amostras - Dataset-Unificado
# Modelo: VITS2 Multi-speaker

# === Configuração do Modelo ===
model:
  name: "VITS2"
  text_encoder_hidden_dim: 192
  latent_dim: 192
  mel_channels: 80
  n_speakers: 52 # Total de falantes únicos no Dataset-Unificado
  speaker_embedding_dim: 512

  # Configurações do generator
  generator_initial_channels: 512
  gin_channels: 512

  # Configurações de inferência
  inference_only: false

# === Configuração de Treinamento ===
training:
  # Parâmetros básicos otimizados para dataset grande
  learning_rate: 5.0e-5 # Muito conservativo para evitar divergência
  batch_size: 12 # Ponto ideal: máximo aproveitamento sem OOM
  max_epochs: 200 # Mais épocas para dataset maior
  accumulate_grad_batches: 3 # Batch efetivo = 36 (12×3)
  max_grad_norm: 0.5 # Gradient clipping mais agressivo

  # Loss weights estabilizados para evitar divergência
  mel_loss_weight: 45.0
  kl_loss_weight: 0.1 # Reduzido drasticamente para estabilidade
  adv_loss_weight: 0.5 # Reduzido para início mais suave
  fm_loss_weight: 1.0 # Reduzido para evitar instabilidade
  duration_loss_weight:
    1.0

    # Otimização
  use_amp: true
  gradient_clip_val: 1.0
  discriminator_update_frequency: 1

  # Scheduler otimizado para treinamento longo
  scheduler:
    name: "ReduceLROnPlateau"
    mode: "min"
    factor: 0.7
    patience: 15 # Mais paciência para dataset grande
    min_lr: 1.0e-6

# === Configuração de Dados ===
data:
  dataset_format: "csv" # Dataset-Unificado usa CSV
  data_dir: "data/generated/Dataset-Unificado"
  metadata_file: "data/generated/Dataset-Unificado/metadata.csv"

  # Preprocessing
  sample_rate: 22050
  n_mels: 80
  n_fft: 1024
  hop_length: 256
  win_length: 1024

  # Data loading otimizado para dataset grande
  num_workers: 6 # Balanceado para processamento eficiente
  pin_memory: true
  persistent_workers: true

  # Divisão de dados
  train_split: 0.95 # 95% para treino
  val_split: 0.05 # 5% para validação

  # Augmentation
  use_augmentation: true
  volume_range: [0.85, 1.15]
  pitch_range: [-1.5, 1.5]

# === Configuração do Monitoramento LLM ===
llm_monitor:
  enabled: true

  # Configuração do LLM
  provider: "openrouter"
  model: "anthropic/claude-3-5-sonnet-20241022"
  api_key_env: "OPENROUTER_API_KEY"
  base_url: "https://openrouter.ai/api/v1"

  # Frequência otimizada para treinamento longo
  monitor_every_epochs: 10 # Análise LLM a cada 10 épocas
  monitor_every_steps: null
  save_analysis_history: true
  max_history_entries: 100

  # Relatórios informativos
  generate_reports: true
  report_every_epochs: 5 # Relatórios informativos a cada 5 épocas

  # Configuração dinâmica
  dynamic_config_path: "configs/training/llm_dynamic_config.yaml"
  backup_original_config: true

  # Ranges seguros
  lr_min: 1.0e-6
  lr_max: 5.0e-3
  lr_change_factor_max: 1.5

  batch_size_min: 8
  batch_size_max: 64
  batch_size_change_factor_max: 1.5

  loss_weight_min: 0.5
  loss_weight_max: 100.0
  loss_weight_change_factor_max: 1.3

  # Segurança
  require_human_approval: true
  critical_change_threshold: 0.25
  max_consecutive_changes: 3

  # Métricas monitoradas
  metrics_to_monitor:
    - "train_loss_total"
    - "train_loss_mel"
    - "train_loss_kl"
    - "train_loss_adv"
    - "train_loss_fm"
    - "train_loss_duration"
    - "val_loss_total"
    - "val_loss_mel"
    - "learning_rate"
    - "grad_norm"
    - "epoch_time"
    - "memory_usage"

  # Limites de alerta
  loss_spike_threshold: 2.5
  grad_norm_threshold: 15.0
  memory_usage_threshold: 0.85

  include_context_epochs: 5

# === Configuração de Logging ===
logging:
  tensorboard:
    save_dir: "logs/tensorboard"
    name: "vits2_dataset_unificado"
    version: null

  wandb:
    enabled: false
    project: "valetts-vits2-unificado"
    name: "vits2-unificado-run"
    tags: ["vits2", "dataset-unificado", "multispeaker"]

  checkpoint:
    dirpath: "checkpoints/vits2_unificado"
    filename: "vits2-unificado-{epoch:03d}-{val_loss_total:.4f}"
    save_top_k: 5
    mode: "min"
    monitor: "val_loss_total"
    save_last: true
    every_n_epochs: 10 # Checkpoint a cada 10 épocas

  early_stopping:
    enabled: true
    monitor: "val_loss_total"
    patience: 50 # Paciência ajustada para monitoramento a cada 10 épocas
    mode: "min"
    min_delta: 0.0005

# === Configuração de Hardware ===
hardware:
  accelerator: "gpu"
  devices: 1
  precision: "16-mixed"
  strategy: "auto"
  sync_batchnorm: true
  benchmark: true
  deterministic: false

# === Configuração de Validação ===
validation:
  val_check_interval: 1.0
  limit_val_batches: 1.0
  num_sanity_val_steps: 5

  # Geração de amostras
  generate_samples: true
  sample_every_n_epochs: 10 # Gerar amostras a cada 10 épocas

# === Configurações Específicas do Dataset ===
dataset_config:
  # Mapeamento de colunas do CSV
  audio_column: "audio_path"
  text_column: "text_normalized"
  speaker_column: "speaker_id"
  duration_column: "duration"

  # Filtros de qualidade
  min_duration: 0.5 # segundos
  max_duration: 15.0 # segundos
  min_text_length: 10
  max_text_length: 500

  # Normalização de texto
  normalize_text: true
  remove_punctuation: false
  lowercase: true
