{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# ğŸ¤ ValeTTS VITS2 English Training - VersÃ£o Otimizada\n",
    "## Treinamento Completo do Modelo VITS2 para InglÃªs no Google Colab\n",
    "\n",
    "**Status**: âœ… **FUNCIONAL** - Sistema completo otimizado para A100\n",
    "\n",
    "### ğŸš€ Recursos Implementados:\n",
    "- âœ… Suporte completo ao inglÃªs com phonemes G2P\n",
    "- âœ… DetecÃ§Ã£o automÃ¡tica de GPU (A100/V100/T4)\n",
    "- âœ… ConfiguraÃ§Ãµes otimizadas por GPU\n",
    "- âœ… Monitor LLM OpenRouter integrado\n",
    "- âœ… TensorBoard em tempo real\n",
    "- âœ… Sistema de checkpoints otimizado (Ãºltimos 2)\n",
    "- âœ… GeraÃ§Ã£o de amostras a cada checkpoint\n",
    "- âœ… Multi-speaker (52 falantes)\n",
    "- âœ… Dataset: 22.910 amostras validadas\n",
    "- âœ… **NOVO**: Download automÃ¡tico do dataset\n",
    "\n",
    "### âš¡ Performance por GPU:\n",
    "- **A100 40GB**: Batch 32 â†’ 5-8 it/s â†’ 4-6 horas\n",
    "- **V100 16GB**: Batch 24 â†’ 3-5 it/s â†’ 6-8 horas\n",
    "- **T4 15GB**: Batch 16 â†’ 1-2 it/s â†’ 12-15 horas\n",
    "\n",
    "### ğŸ“‹ Ordem de ExecuÃ§Ã£o:\n",
    "1. **ConfiguraÃ§Ã£o da API OpenRouter** (opcional)\n",
    "2. **ConfiguraÃ§Ã£o do Modo de Treinamento**\n",
    "3. **Setup do Sistema e Clonagem**\n",
    "4. **InstalaÃ§Ã£o de DependÃªncias**\n",
    "5. **ğŸ†• Download AutomÃ¡tico do Dataset** â† **NOVA CÃ‰LULA**\n",
    "6. **VerificaÃ§Ã£o Completa do Dataset**\n",
    "7. **ConfiguraÃ§Ã£o DinÃ¢mica do Modelo**\n",
    "8. **InicializaÃ§Ã£o do TensorBoard**\n",
    "9. **ExecuÃ§Ã£o do Treinamento**\n",
    "10. **Download dos Resultados**\n",
    "\n",
    "### ğŸ“¥ Sistema de Download AutomÃ¡tico:\n",
    "- âœ… DetecÃ§Ã£o automÃ¡tica se dataset jÃ¡ estÃ¡ completo\n",
    "- âœ… Download apenas dos arquivos de Ã¡udio (se metadata jÃ¡ existe)\n",
    "- âœ… MÃºltiplos mÃ©todos: Google Drive, HuggingFace, GitHub\n",
    "- âœ… Fallback para Google Drive manual\n",
    "- âœ… VerificaÃ§Ã£o de integridade automÃ¡tica\n",
    "\n",
    "### ğŸ”§ Para Usar o Download AutomÃ¡tico:\n",
    "1. **OpÃ§Ã£o 1 (Recomendada)**: Copie seu dataset para `/content/drive/MyDrive/ValeTTS-Colab/Dataset-Unificado.tar.gz`\n",
    "2. **OpÃ§Ã£o 2**: Configure URLs reais na cÃ©lula 4.5\n",
    "3. **OpÃ§Ã£o 3**: Execute a cÃ©lula 4.5 e siga as instruÃ§Ãµes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CONFIGURAÃ‡ÃƒO DA API OPENROUTER (OPCIONAL) ===\n",
    "# Monitor LLM para otimizaÃ§Ã£o inteligente do treinamento\n",
    "\n",
    "import os\n",
    "import getpass\n",
    "\n",
    "# ConfiguraÃ§Ã£o da API OpenRouter\n",
    "ENABLE_LLM_MONITOR = True # @param {type:\"boolean\"}\n",
    "\n",
    "if ENABLE_LLM_MONITOR:\n",
    "    print(\"ğŸ¤– Configurando Monitor LLM com OpenRouter...\")\n",
    "    print(\"ğŸ“‹ O monitor LLM analisa o progresso do treinamento e sugere ajustes automÃ¡ticos\")\n",
    "    print(\"ğŸ’¡ Opcional: deixe em branco para desabilitar\")\n",
    "\n",
    "    # Input da API key (campo seguro)\n",
    "    OPENROUTER_API_KEY = getpass.getpass(\"ğŸ”‘ Digite sua API Key do OpenRouter (ou Enter para pular): \")\n",
    "\n",
    "    if OPENROUTER_API_KEY and OPENROUTER_API_KEY.strip():\n",
    "        # Configurar variÃ¡vel de ambiente\n",
    "        os.environ['OPENROUTER_API_KEY'] = OPENROUTER_API_KEY.strip()\n",
    "\n",
    "        # Testar conexÃ£o\n",
    "        try:\n",
    "            import requests\n",
    "            test_url = \"https://openrouter.ai/api/v1/models\"\n",
    "            headers = {\n",
    "                \"Authorization\": f\"Bearer {OPENROUTER_API_KEY.strip()}\",\n",
    "                \"Content-Type\": \"application/json\"\n",
    "            }\n",
    "\n",
    "            response = requests.get(test_url, headers=headers, timeout=10)\n",
    "            if response.status_code == 200:\n",
    "                print(\"âœ… API Key vÃ¡lida - Monitor LLM habilitado\")\n",
    "                print(\"ğŸ“Š Funcionalidades ativas:\")\n",
    "                print(\"   - AnÃ¡lise automÃ¡tica a cada 10 Ã©pocas\")\n",
    "                print(\"   - SugestÃµes de ajuste de learning rate\")\n",
    "                print(\"   - DetecÃ§Ã£o de problemas de convergÃªncia\")\n",
    "                print(\"   - RelatÃ³rios detalhados de progresso\")\n",
    "\n",
    "                # Mostrar modelos disponÃ­veis\n",
    "                models = response.json().get('data', [])\n",
    "                claude_models = [m for m in models if 'claude' in m.get('id', '').lower()]\n",
    "                if claude_models:\n",
    "                    print(f\"ğŸ§  Modelo recomendado: {claude_models[0].get('id', 'claude-3-5-sonnet')}\")\n",
    "            else:\n",
    "                print(f\"âš ï¸ Erro na validaÃ§Ã£o da API: {response.status_code}\")\n",
    "                print(\"ğŸ”§ Monitor LLM serÃ¡ desabilitado para este treinamento\")\n",
    "                ENABLE_LLM_MONITOR = False\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Erro ao testar API: {e}\")\n",
    "            print(\"ğŸ”§ Monitor LLM serÃ¡ desabilitado para este treinamento\")\n",
    "            ENABLE_LLM_MONITOR = False\n",
    "    else:\n",
    "        print(\"â„¹ï¸ API Key nÃ£o fornecida - Monitor LLM desabilitado\")\n",
    "        ENABLE_LLM_MONITOR = False\n",
    "else:\n",
    "    print(\"ğŸ”§ Monitor LLM desabilitado por configuraÃ§Ã£o\")\n",
    "    ENABLE_LLM_MONITOR = False\n",
    "\n",
    "# Salvar configuraÃ§Ã£o para uso posterior\n",
    "LLM_CONFIG = {\n",
    "    'enabled': ENABLE_LLM_MONITOR,\n",
    "    'provider': 'openrouter',\n",
    "    'model': 'anthropic/claude-3-5-sonnet-20241022',\n",
    "    'monitor_every_epochs': 10,\n",
    "    'api_key_configured': bool(os.environ.get('OPENROUTER_API_KEY'))\n",
    "}\n",
    "\n",
    "print(f\"\\nğŸ¯ ConfiguraÃ§Ã£o LLM: {'âœ… Ativo' if ENABLE_LLM_MONITOR else 'âŒ Desabilitado'}\")\n",
    "\n",
    "if not ENABLE_LLM_MONITOR:\n",
    "    print(\"\\nğŸ’¡ Para habilitar o Monitor LLM:\")\n",
    "    print(\"1. Crie uma conta em https://openrouter.ai\")\n",
    "    print(\"2. Obtenha sua API Key\")\n",
    "    print(\"3. Execute esta cÃ©lula novamente e forneÃ§a a key\")\n",
    "    print(\"4. O monitor ajudarÃ¡ a otimizar seu treinamento automaticamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CONFIGURAÃ‡ÃƒO DO MODO DE TREINAMENTO ===\n",
    "# ConfiguraÃ§Ã£o otimizada para A100 40GB VRAM\n",
    "DEBUG_MODE = False # @param {type:\"boolean\"}\n",
    "USE_DRIVE = True # @param {type:\"boolean\"}\n",
    "MOUNT_DRIVE = True # @param {type:\"boolean\"}\n",
    "\n",
    "# Detectar GPU automaticamente e ajustar configuraÃ§Ã£o\n",
    "import subprocess\n",
    "def get_gpu_info():\n",
    "    try:\n",
    "        result = subprocess.run(['nvidia-smi', '--query-gpu=name,memory.total', '--format=csv,noheader,nounits'],\n",
    "                              capture_output=True, text=True)\n",
    "        if result.returncode == 0:\n",
    "            gpu_info = result.stdout.strip().split('\\n')[0]\n",
    "            gpu_name, gpu_memory = gpu_info.split(', ')\n",
    "            return gpu_name.strip(), int(gpu_memory.strip())\n",
    "    except:\n",
    "        pass\n",
    "    return \"Unknown\", 0\n",
    "\n",
    "gpu_name, gpu_memory = get_gpu_info()\n",
    "print(f\"ğŸ® GPU Detectada: {gpu_name}\")\n",
    "print(f\"ğŸ’¾ VRAM: {gpu_memory} MB\")\n",
    "\n",
    "# ConfiguraÃ§Ã£o automÃ¡tica baseada na GPU\n",
    "if \"A100\" in gpu_name and gpu_memory >= 40000:\n",
    "    print(\"ğŸš€ MODO A100: ConfiguraÃ§Ã£o de alta performance\")\n",
    "    BATCH_SIZE = 32\n",
    "    CONFIG_NAME = \"vits2_english_a100_optimized\"\n",
    "    EXPECTED_PERFORMANCE = \"5-8 it/s\"\n",
    "    ESTIMATED_TIME = \"4-6 horas\"\n",
    "elif \"V100\" in gpu_name:\n",
    "    print(\"âš¡ MODO V100: ConfiguraÃ§Ã£o otimizada\")\n",
    "    BATCH_SIZE = 24\n",
    "    CONFIG_NAME = \"vits2_english_production\"\n",
    "    EXPECTED_PERFORMANCE = \"3-5 it/s\"\n",
    "    ESTIMATED_TIME = \"6-8 horas\"\n",
    "elif \"T4\" in gpu_name:\n",
    "    print(\"ğŸ“± MODO T4: ConfiguraÃ§Ã£o conservativa\")\n",
    "    BATCH_SIZE = 16\n",
    "    CONFIG_NAME = \"vits2_english_production\"\n",
    "    EXPECTED_PERFORMANCE = \"1-2 it/s\"\n",
    "    ESTIMATED_TIME = \"12-15 horas\"\n",
    "else:\n",
    "    print(\"âš ï¸ GPU nÃ£o detectada, usando configuraÃ§Ã£o padrÃ£o\")\n",
    "    BATCH_SIZE = 16\n",
    "    CONFIG_NAME = \"vits2_english_production\"\n",
    "    EXPECTED_PERFORMANCE = \"1-2 it/s\"\n",
    "    ESTIMATED_TIME = \"12-15 horas\"\n",
    "\n",
    "# ConfiguraÃ§Ã£o do treinamento\n",
    "if DEBUG_MODE:\n",
    "    print(\"ğŸ› MODO DEBUG: Treinamento rÃ¡pido para teste\")\n",
    "    EPOCHS = 3\n",
    "    MAX_SAMPLES = 100\n",
    "    CONFIG_NAME = \"vits2_english_debug\"\n",
    "    BATCH_SIZE = min(BATCH_SIZE, 8)  # Reduzido para debug\n",
    "else:\n",
    "    print(\"ğŸš€ MODO PRODUÃ‡ÃƒO: Treinamento completo\")\n",
    "    EPOCHS = 200\n",
    "    MAX_SAMPLES = None\n",
    "\n",
    "print(f\"ğŸ“Š ConfiguraÃ§Ã£o: {CONFIG_NAME}\")\n",
    "print(f\"ğŸ”„ Ã‰pocas: {EPOCHS}\")\n",
    "print(f\"ğŸ“¦ Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"ğŸ“ˆ Amostras: {MAX_SAMPLES if MAX_SAMPLES else 'Todas (22.910)'}\")\n",
    "print(f\"âš¡ Performance esperada: {EXPECTED_PERFORMANCE}\")\n",
    "print(f\"â±ï¸ Tempo estimado: {ESTIMATED_TIME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CONFIGURAÃ‡ÃƒO DO SISTEMA E CLONAGEM ===\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "\n",
    "def run_command(cmd, description):\n",
    "    \"\"\"Executa comando com output em tempo real.\"\"\"\n",
    "    print(f\"ğŸ”„ {description}\")\n",
    "    process = subprocess.Popen(\n",
    "        cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT,\n",
    "        universal_newlines=True, bufsize=1\n",
    "    )\n",
    "\n",
    "    for line in process.stdout:\n",
    "        print(line.rstrip())\n",
    "\n",
    "    process.wait()\n",
    "    if process.returncode != 0:\n",
    "        raise RuntimeError(f\"Comando falhou: {cmd}\")\n",
    "    print(f\"âœ… {description} - ConcluÃ­do\\n\")\n",
    "\n",
    "# Montar Google Drive se necessÃ¡rio\n",
    "if MOUNT_DRIVE:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    if USE_DRIVE:\n",
    "        drive_path = '/content/drive/MyDrive/ValeTTS-Colab'\n",
    "        os.makedirs(drive_path, exist_ok=True)\n",
    "        os.chdir(drive_path)\n",
    "        print(f\"ğŸ“ DiretÃ³rio de trabalho: {drive_path}\")\n",
    "\n",
    "# Verificar GPU\n",
    "run_command(\"nvidia-smi\", \"Verificando GPU disponÃ­vel\")\n",
    "\n",
    "# Clonar repositÃ³rio\n",
    "if not os.path.exists('ValeTTS'):\n",
    "    run_command(\n",
    "        \"git clone https://github.com/wallaceblaia/ValeTTS-Colab.git ValeTTS\",\n",
    "        \"Clonando repositÃ³rio ValeTTS\"\n",
    "    )\n",
    "else:\n",
    "    print(\"ğŸ“ RepositÃ³rio jÃ¡ existe\")\n",
    "\n",
    "os.chdir('ValeTTS')\n",
    "run_command(\"git pull origin main\", \"Atualizando repositÃ³rio\")\n",
    "print(f\"ğŸ“ DiretÃ³rio atual: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === DOWNLOAD AUTOMÃTICO DO DATASET INGLÃŠS ===\n",
    "# CÃ©lula 4.5: Download e descompactaÃ§Ã£o automÃ¡tica do dataset\n",
    "\n",
    "import os\n",
    "import zipfile\n",
    "import tarfile\n",
    "import requests\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "print(\"ğŸ“¥ SISTEMA DE DOWNLOAD AUTOMÃTICO DO DATASET\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ConfiguraÃ§Ã£o de caminhos\n",
    "dataset_path = \"data/generated/Dataset-Unificado\"\n",
    "metadata_file = f\"{dataset_path}/metadata.csv\"\n",
    "audio_dir = f\"{dataset_path}/audio\"\n",
    "audio_raw_dir = f\"{dataset_path}/audio/raw\"\n",
    "\n",
    "# Caminho do arquivo no Google Drive (CORRETO: ValeTTS-Colab)\n",
    "drive_dataset_file = \"/content/drive/MyDrive/ValeTTS-Colab/Dataset-Unificado.tar.gz\"\n",
    "drive_dataset_fallback = \"/content/drive/MyDrive/ValeTTS-Colab/data/generated/Dataset-Unificado\"\n",
    "\n",
    "print(f\"ğŸ“ Caminhos configurados:\")\n",
    "print(f\"   Dataset: {dataset_path}\")\n",
    "print(f\"   Metadata: {metadata_file}\")\n",
    "print(f\"   Ãudio: {audio_raw_dir}\")\n",
    "print(f\"   Drive TAR.GZ: {drive_dataset_file}\")\n",
    "\n",
    "# FunÃ§Ã£o para extrair arquivo TAR.GZ\n",
    "def extract_tarfile(tar_path, extract_to):\n",
    "    \"\"\"Extrai arquivo TAR.GZ com barra de progresso.\"\"\"\n",
    "    print(f\"ğŸ“¦ Extraindo arquivo TAR.GZ: {tar_path}\")\n",
    "    print(f\"ğŸ“ Destino: {extract_to}\")\n",
    "\n",
    "    try:\n",
    "        # Verificar tamanho do arquivo\n",
    "        file_size = os.path.getsize(tar_path)\n",
    "        print(f\"   ğŸ“Š Tamanho do arquivo: {file_size / 1024 / 1024:.1f} MB\")\n",
    "\n",
    "        with tarfile.open(tar_path, 'r:gz') as tar_ref:\n",
    "            # Mostrar conteÃºdo do TAR\n",
    "            members = tar_ref.getmembers()\n",
    "            print(f\"   ğŸ“‹ Arquivos no TAR.GZ: {len(members)}\")\n",
    "\n",
    "            # Contar arquivos WAV\n",
    "            wav_files = [m for m in members if m.name.endswith('.wav')]\n",
    "            print(f\"   ğŸµ Arquivos WAV encontrados: {len(wav_files)}\")\n",
    "\n",
    "            # Extrair todos os arquivos\n",
    "            print(f\"   ğŸ”„ Iniciando extraÃ§Ã£o...\")\n",
    "            tar_ref.extractall(extract_to)\n",
    "\n",
    "        print(f\"âœ… ExtraÃ§Ã£o concluÃ­da com sucesso!\")\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Erro na extraÃ§Ã£o: {e}\")\n",
    "        return False\n",
    "\n",
    "# FunÃ§Ã£o para extrair ZIP (fallback)\n",
    "def extract_zip(zip_path, extract_to):\n",
    "    \"\"\"Extrai arquivo ZIP.\"\"\"\n",
    "    print(f\"ğŸ“¦ Extraindo ZIP: {zip_path}\")\n",
    "    print(f\"ğŸ“ Destino: {extract_to}\")\n",
    "\n",
    "    try:\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            file_list = zip_ref.namelist()\n",
    "            print(f\"   ğŸ“‹ Arquivos no ZIP: {len(file_list)}\")\n",
    "            zip_ref.extractall(extract_to)\n",
    "\n",
    "        print(f\"âœ… ExtraÃ§Ã£o ZIP concluÃ­da!\")\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Erro na extraÃ§Ã£o ZIP: {e}\")\n",
    "        return False\n",
    "\n",
    "# ESTRATÃ‰GIA CORRIGIDA: Primeiro verificar o arquivo TAR.GZ, depois extrair, e sÃ³ entÃ£o verificar os arquivos\n",
    "\n",
    "print(\"\\nğŸ“¥ INICIANDO DOWNLOAD E DESCOMPACTAÃ‡ÃƒO...\")\n",
    "\n",
    "# Criar diretÃ³rio de destino primeiro\n",
    "os.makedirs(\"data/generated\", exist_ok=True)\n",
    "\n",
    "# ETAPA 1: Verificar e processar arquivo TAR.GZ PRIMEIRO\n",
    "if os.path.exists(drive_dataset_file):\n",
    "    print(f\"\\nğŸ¯ MÃ‰TODO 1: Arquivo TAR.GZ encontrado no Google Drive!\")\n",
    "    print(f\"   ğŸ“ Local: {drive_dataset_file}\")\n",
    "\n",
    "    # Verificar tamanho do arquivo\n",
    "    file_size_mb = os.path.getsize(drive_dataset_file) / 1024 / 1024\n",
    "    print(f\"   ğŸ“Š Tamanho: {file_size_mb:.1f} MB\")\n",
    "\n",
    "    # Verificar se jÃ¡ foi extraÃ­do no local correto\n",
    "    if os.path.exists(dataset_path) and os.path.exists(metadata_file) and os.path.exists(audio_raw_dir):\n",
    "        # Contar arquivos existentes\n",
    "        wav_files_check = [f for f in os.listdir(audio_raw_dir) if f.endswith('.wav')]\n",
    "        existing_wav_count = len(wav_files_check) if wav_files_check else 0\n",
    "\n",
    "        print(f\"\\nğŸ” Dataset jÃ¡ parece estar extraÃ­do:\")\n",
    "        print(f\"   ğŸ“„ Metadata: {'âœ…' if os.path.exists(metadata_file) else 'âŒ'}\")\n",
    "        print(f\"   ğŸµ Arquivos de Ã¡udio: {existing_wav_count:,} arquivos WAV\")\n",
    "\n",
    "        # Se temos poucos arquivos, forÃ§ar re-extraÃ§Ã£o\n",
    "        if existing_wav_count < 20000:  # Esperamos ~22.910\n",
    "            print(f\"   âš ï¸ Poucos arquivos detectados, forÃ§ando re-extraÃ§Ã£o...\")\n",
    "            # Remover diretÃ³rio existente para re-extrair\n",
    "            if os.path.exists(dataset_path):\n",
    "                if os.path.islink(dataset_path):\n",
    "                    os.unlink(dataset_path)\n",
    "                elif os.path.isdir(dataset_path):\n",
    "                    shutil.rmtree(dataset_path)\n",
    "        else:\n",
    "            print(f\"   âœ… Dataset parece completo, pulando extraÃ§Ã£o\")\n",
    "\n",
    "    # Extrair se necessÃ¡rio\n",
    "    if not (os.path.exists(dataset_path) and os.path.exists(metadata_file) and os.path.exists(audio_raw_dir)):\n",
    "        print(f\"\\nğŸ”„ Extraindo dataset do arquivo TAR.GZ...\")\n",
    "        if extract_tarfile(drive_dataset_file, \"data/generated\"):\n",
    "            print(\"âœ… Sucesso! Dataset extraÃ­do do Google Drive TAR.GZ\")\n",
    "\n",
    "            # Verificar se a extraÃ§Ã£o criou a estrutura correta\n",
    "            extracted_paths = [\n",
    "                \"data/generated/Dataset-Unificado\",\n",
    "                \"data/generated/Dataset-Unificado/metadata.csv\",\n",
    "                \"data/generated/Dataset-Unificado/audio/raw\"\n",
    "            ]\n",
    "\n",
    "            extraction_success = True\n",
    "            for check_path in extracted_paths:\n",
    "                if os.path.exists(check_path):\n",
    "                    if os.path.isfile(check_path):\n",
    "                        size = os.path.getsize(check_path)\n",
    "                        print(f\"   âœ… {check_path} ({size:,} bytes)\")\n",
    "                    else:\n",
    "                        items = len(os.listdir(check_path)) if os.path.isdir(check_path) else 0\n",
    "                        print(f\"   âœ… {check_path} ({items:,} itens)\")\n",
    "                else:\n",
    "                    print(f\"   âŒ {check_path} - NÃƒO ENCONTRADO\")\n",
    "                    extraction_success = False\n",
    "\n",
    "            if extraction_success:\n",
    "                # Contar arquivos WAV extraÃ­dos\n",
    "                if os.path.exists(audio_raw_dir):\n",
    "                    wav_files = [f for f in os.listdir(audio_raw_dir) if f.endswith('.wav')]\n",
    "                    wav_count = len(wav_files)\n",
    "                    print(f\"\\nğŸµ Arquivos WAV extraÃ­dos: {wav_count:,}\")\n",
    "        else:\n",
    "            print(\"âŒ Falha na extraÃ§Ã£o do TAR.GZ\")\n",
    "\n",
    "elif os.path.exists(drive_dataset_fallback):\n",
    "    # MÃ‰TODO 2: DiretÃ³rio jÃ¡ extraÃ­do no Google Drive (FALLBACK)\n",
    "    print(f\"\\nğŸ”„ MÃ‰TODO 2: DiretÃ³rio jÃ¡ extraÃ­do encontrado no Google Drive...\")\n",
    "    print(f\"âœ… Dataset encontrado no Drive (jÃ¡ extraÃ­do)!\")\n",
    "    print(f\"   ğŸ“ Local: {drive_dataset_fallback}\")\n",
    "\n",
    "    # Mostrar conteÃºdo do Drive\n",
    "    drive_contents = os.listdir(drive_dataset_fallback)\n",
    "    print(f\"   ğŸ“‹ ConteÃºdo: {drive_contents}\")\n",
    "\n",
    "    print(f\"\\nğŸ”— Criando estrutura local...\")\n",
    "\n",
    "    # Verificar se jÃ¡ existe link/diretÃ³rio\n",
    "    if os.path.exists(dataset_path):\n",
    "        if os.path.islink(dataset_path):\n",
    "            print(\"   ğŸ—‘ï¸ Removendo link simbÃ³lico anterior...\")\n",
    "            os.unlink(dataset_path)\n",
    "        elif os.path.isdir(dataset_path):\n",
    "            print(\"   ğŸ—‘ï¸ Removendo diretÃ³rio anterior...\")\n",
    "            shutil.rmtree(dataset_path)\n",
    "\n",
    "    print(\"   ğŸ”— Criando link simbÃ³lico...\")\n",
    "    os.symlink(drive_dataset_fallback, dataset_path)\n",
    "    print(\"   âœ… Link criado com sucesso!\")\n",
    "\n",
    "else:\n",
    "    print(f\"\\nâŒ NENHUM MÃ‰TODO FUNCIONOU: Dataset nÃ£o encontrado\")\n",
    "    print(f\"   ğŸ“ TAR.GZ procurado em: {drive_dataset_file}\")\n",
    "    print(f\"   ğŸ“ DiretÃ³rio procurado em: {drive_dataset_fallback}\")\n",
    "\n",
    "# ETAPA 2: Verificar status final APÃ“S processamento\n",
    "print(\"\\nğŸ” Verificando status final do dataset...\")\n",
    "\n",
    "metadata_exists = os.path.exists(metadata_file)\n",
    "audio_exists = False\n",
    "wav_count = 0\n",
    "\n",
    "if os.path.exists(audio_raw_dir):\n",
    "    wav_files = [f for f in os.listdir(audio_raw_dir) if f.endswith('.wav')]\n",
    "    wav_count = len(wav_files)\n",
    "    audio_exists = wav_count > 0\n",
    "\n",
    "print(f\"   ğŸ“„ Metadata: {'âœ…' if metadata_exists else 'âŒ'}\")\n",
    "print(f\"   ğŸµ Arquivos de Ã¡udio: {'âœ…' if audio_exists else 'âŒ'} ({wav_count:,} arquivos WAV)\")\n",
    "\n",
    "if metadata_exists and audio_exists:\n",
    "    print(\"\\nâœ… DATASET ESTÃ COMPLETO E PRONTO!\")\n",
    "    print(\"ğŸ¯ Pode prosseguir para a prÃ³xima cÃ©lula\")\n",
    "else:\n",
    "    print(\"\\nâŒ DATASET AINDA INCOMPLETO\")\n",
    "\n",
    "    # MÃ‰TODO 3: Procurar arquivos TAR.GZ locais (ÃšLTIMO RECURSO)\n",
    "    print(f\"\\nğŸ”„ MÃ‰TODO 3: Procurando arquivos TAR.GZ locais...\")\n",
    "\n",
    "    possible_local_files = [\n",
    "        \"Dataset-Unificado.tar.gz\",\n",
    "        \"ValeTTS-Colab-Dataset-Unificado.tar.gz\",\n",
    "        \"/content/Dataset-Unificado.tar.gz\"\n",
    "    ]\n",
    "\n",
    "    found_local = False\n",
    "    for local_file in possible_local_files:\n",
    "        if os.path.exists(local_file):\n",
    "            print(f\"âœ… Arquivo TAR.GZ local encontrado: {local_file}\")\n",
    "            found_local = True\n",
    "\n",
    "            if extract_tarfile(local_file, \"data/generated\"):\n",
    "                print(\"âœ… Sucesso com arquivo local!\")\n",
    "                # Atualizar status apÃ³s extraÃ§Ã£o local\n",
    "                metadata_exists = os.path.exists(metadata_file)\n",
    "                if os.path.exists(audio_raw_dir):\n",
    "                    wav_files = [f for f in os.listdir(audio_raw_dir) if f.endswith('.wav')]\n",
    "                    wav_count = len(wav_files)\n",
    "                    audio_exists = wav_count > 0\n",
    "                break\n",
    "            else:\n",
    "                print(\"âŒ Falha na extraÃ§Ã£o do arquivo local\")\n",
    "\n",
    "    if not found_local:\n",
    "        print(\"âŒ Nenhum arquivo TAR.GZ encontrado localmente\")\n",
    "\n",
    "# VerificaÃ§Ã£o final\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“Š STATUS FINAL DO DOWNLOAD:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "final_metadata = os.path.exists(metadata_file)\n",
    "final_audio = os.path.exists(audio_raw_dir)\n",
    "final_wav_count = 0\n",
    "\n",
    "if final_audio:\n",
    "    wav_files = [f for f in os.listdir(audio_raw_dir) if f.endswith('.wav')]\n",
    "    final_wav_count = len(wav_files)\n",
    "\n",
    "print(f\"ğŸ“„ Metadata: {'âœ…' if final_metadata else 'âŒ'}\")\n",
    "print(f\"ğŸ“ Pasta Ã¡udio: {'âœ…' if final_audio else 'âŒ'}\")\n",
    "print(f\"ğŸµ Arquivos WAV: {final_wav_count:,}\")\n",
    "\n",
    "if final_metadata and final_wav_count > 0:\n",
    "    print(f\"\\nğŸš€ DATASET PRONTO PARA VERIFICAÃ‡ÃƒO!\")\n",
    "    print(f\"âœ… Execute a prÃ³xima cÃ©lula para verificaÃ§Ã£o completa\")\n",
    "    print(f\"ğŸ“Š Arquivos detectados: {final_wav_count:,}\")\n",
    "\n",
    "    # Mostrar estatÃ­sticas bÃ¡sicas\n",
    "    if final_wav_count > 0:\n",
    "        total_size = 0\n",
    "        sample_sizes = []\n",
    "        for i, wav_file in enumerate(os.listdir(audio_raw_dir)[:100]):  # Amostra de 100\n",
    "            if wav_file.endswith('.wav'):\n",
    "                wav_path = os.path.join(audio_raw_dir, wav_file)\n",
    "                size = os.path.getsize(wav_path)\n",
    "                total_size += size\n",
    "                sample_sizes.append(size)\n",
    "\n",
    "        if sample_sizes:\n",
    "            avg_size = sum(sample_sizes) / len(sample_sizes)\n",
    "            estimated_total = avg_size * final_wav_count\n",
    "            print(f\"\\nğŸ“Š EstatÃ­sticas dos arquivos:\")\n",
    "            print(f\"   ğŸ“ Tamanho mÃ©dio: {avg_size / 1024:.1f} KB por arquivo\")\n",
    "            print(f\"   ğŸ’¾ Tamanho estimado total: {estimated_total / 1024 / 1024:.1f} MB\")\n",
    "\n",
    "elif final_metadata and final_wav_count == 0:\n",
    "    print(f\"\\nâš ï¸ METADATA OK, MAS SEM ARQUIVOS DE ÃUDIO\")\n",
    "    print(f\"ğŸ”§ SoluÃ§Ãµes:\")\n",
    "    print(f\"   1. Verifique se o arquivo TAR.GZ estÃ¡ em: {drive_dataset_file}\")\n",
    "    print(f\"   2. Ou coloque o dataset jÃ¡ extraÃ­do em: {drive_dataset_fallback}\")\n",
    "else:\n",
    "    print(f\"\\nâŒ DATASET AINDA INCOMPLETO\")\n",
    "    print(f\"\")\n",
    "    print(f\"ğŸ”§ SOLUÃ‡Ã•ES PARA CORRIGIR:\")\n",
    "    print(f\"   1. MÃ‰TODO RECOMENDADO - Arquivo TAR.GZ:\")\n",
    "    print(f\"      â€¢ Coloque Dataset-Unificado.tar.gz no Google Drive:\")\n",
    "    print(f\"        {drive_dataset_file}\")\n",
    "    print(f\"      â€¢ Execute esta cÃ©lula novamente\")\n",
    "    print(f\"      â€¢ O sistema extrairÃ¡ automaticamente para o local correto\")\n",
    "    print(f\"\")\n",
    "    print(f\"   2. MÃ‰TODO ALTERNATIVO - Pasta jÃ¡ extraÃ­da:\")\n",
    "    print(f\"      â€¢ Extraia manualmente e coloque em:\")\n",
    "    print(f\"        {drive_dataset_fallback}\")\n",
    "    print(f\"      â€¢ Execute esta cÃ©lula novamente\")\n",
    "\n",
    "print(f\"\\nğŸ’¡ CONFIGURAÃ‡ÃƒO ATUAL DO SEU GOOGLE DRIVE:\")\n",
    "print(f\"   ğŸ“ Arquivo TAR.GZ: {'âœ…' if os.path.exists(drive_dataset_file) else 'âŒ'} {drive_dataset_file}\")\n",
    "print(f\"   ğŸ“ Pasta extraÃ­da: {'âœ…' if os.path.exists(drive_dataset_fallback) else 'âŒ'} {drive_dataset_fallback}\")\n",
    "\n",
    "if not os.path.exists(drive_dataset_file) and not os.path.exists(drive_dataset_fallback):\n",
    "    print(f\"\\nâš ï¸ ATENÃ‡ÃƒO - DATASET NÃƒO ENCONTRADO!\")\n",
    "    print(f\"\")\n",
    "    print(f\"ğŸ“‹ PASSOS PARA RESOLVER:\")\n",
    "    print(f\"   1. Localize o arquivo Dataset-Unificado.tar.gz\")\n",
    "    print(f\"   2. FaÃ§a upload para o Google Drive no caminho:\")\n",
    "    print(f\"      /content/drive/MyDrive/ValeTTS-Colab/Dataset-Unificado.tar.gz\")\n",
    "    print(f\"   3. Execute esta cÃ©lula novamente\")\n",
    "    print(f\"   4. O sistema extrairÃ¡ automaticamente para data/generated/\")\n",
    "    print(f\"\")\n",
    "    print(f\"ğŸ“‚ ESTRUTURA CORRETA NO GOOGLE DRIVE:\")\n",
    "    print(f\"   ğŸ“ /content/drive/MyDrive/ValeTTS-Colab/\")\n",
    "    print(f\"   ğŸ“¦ â”œâ”€â”€ Dataset-Unificado.tar.gz  â† COLOQUE AQUI\")\n",
    "    print(f\"   ğŸ“‚ â”œâ”€â”€ ValeTTS/  (cÃ³digo)\")\n",
    "    print(f\"   ğŸ“‚ â””â”€â”€ outros arquivos...\")\n",
    "    print(f\"\")\n",
    "    print(f\"âš¡ IMPORTANTE: O dataset serÃ¡ extraÃ­do LOCALMENTE em data/generated/\")\n",
    "    print(f\"   Isso evita problemas de performance e espaÃ§o no Google Drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === INSTALAÃ‡ÃƒO DE DEPENDÃŠNCIAS ESPECÃFICAS PARA INGLÃŠS ===\n",
    "\n",
    "print(\"ğŸ”§ Instalando dependÃªncias do sistema para inglÃªs...\")\n",
    "\n",
    "# Instalar dependÃªncias do sistema para inglÃªs\n",
    "system_deps = [\n",
    "    \"apt-get update\",\n",
    "    \"apt-get install -y espeak espeak-data libespeak1 libespeak-dev\",\n",
    "    \"apt-get install -y ffmpeg sox libsox-fmt-all\",\n",
    "    \"apt-get install -y language-pack-en\",  # Suporte especÃ­fico para inglÃªs\n",
    "]\n",
    "\n",
    "for cmd in system_deps:\n",
    "    run_command(cmd, f\"Sistema: {cmd.split()[-1]}\")\n",
    "\n",
    "print(\"ğŸ“¦ Instalando dependÃªncias Python...\")\n",
    "\n",
    "# Lista de dependÃªncias Python essenciais\n",
    "essential_deps = [\n",
    "    \"pip install phonemizer==3.3.0\",  # G2P essencial\n",
    "    \"pip install num2words\",          # ConversÃ£o nÃºmeros para inglÃªs\n",
    "    \"pip install inflect\",            # PluralizaÃ§Ã£o e ordinais em inglÃªs\n",
    "]\n",
    "\n",
    "# Lista de dependÃªncias opcionais\n",
    "optional_deps = [\n",
    "    (\"pip install eSpeak\", \"eSpeak Python interface\"),\n",
    "    (\"pip install espeak-ng\", \"eSpeak-NG alternativa\"),\n",
    "]\n",
    "\n",
    "# Instalar dependÃªncias essenciais\n",
    "for cmd in essential_deps:\n",
    "    run_command(cmd, f\"Python essencial: {cmd.split()[-1]}\")\n",
    "\n",
    "# Tentar instalar dependÃªncias opcionais\n",
    "for cmd, desc in optional_deps:\n",
    "    try:\n",
    "        run_command(cmd, f\"Python opcional: {desc}\")\n",
    "        break  # Se uma funcionar, nÃ£o precisamos das outras\n",
    "    except RuntimeError:\n",
    "        print(f\"âš ï¸ {desc} nÃ£o disponÃ­vel, tentando prÃ³xima...\")\n",
    "\n",
    "print(\"ğŸ”§ Instalando ValeTTS...\")\n",
    "# Instalar ValeTTS em modo desenvolvimento\n",
    "run_command(\"pip install -e .\", \"Instalando ValeTTS em modo desenvolvimento\")\n",
    "\n",
    "# Verificar instalaÃ§Ãµes crÃ­ticas\n",
    "print(\"ğŸ§ª Verificando instalaÃ§Ãµes...\")\n",
    "\n",
    "# VerificaÃ§Ãµes essenciais\n",
    "essential_checks = [\n",
    "    (\"python -c 'import phonemizer; print(f\\\"âœ… Phonemizer: {phonemizer.__version__}\\\")'\", \"Phonemizer\"),\n",
    "    (\"python -c 'import valetts; print(f\\\"âœ… ValeTTS instalado\\\")'\", \"ValeTTS\"),\n",
    "]\n",
    "\n",
    "# VerificaÃ§Ãµes opcionais\n",
    "optional_checks = [\n",
    "    (\"espeak --version\", \"ESpeak sistema\"),\n",
    "    (\"python -c 'from valetts.data.preprocessing.text_en import EnglishTextPreprocessor; print(\\\"âœ… EnglishTextPreprocessor disponÃ­vel\\\")'\", \"Processador inglÃªs\"),\n",
    "]\n",
    "\n",
    "# Executar verificaÃ§Ãµes essenciais\n",
    "for cmd, desc in essential_checks:\n",
    "    run_command(cmd, f\"Verificar {desc}\")\n",
    "\n",
    "# Executar verificaÃ§Ãµes opcionais\n",
    "for cmd, desc in optional_checks:\n",
    "    try:\n",
    "        run_command(cmd, f\"Verificar {desc}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Aviso: {desc} - {e}\")\n",
    "\n",
    "# Teste robusto do processador de inglÃªs\n",
    "print(\"ğŸ§ª Testando processamento de inglÃªs...\")\n",
    "\n",
    "test_code_robust = '''\n",
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(\"ğŸ” Testando importaÃ§Ãµes...\")\n",
    "\n",
    "# Teste 1: Importar ValeTTS\n",
    "try:\n",
    "    import valetts\n",
    "    print(\"âœ… ValeTTS importado\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ ValeTTS: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Teste 2: Importar processador de texto inglÃªs\n",
    "try:\n",
    "    from valetts.data.preprocessing.text_en import EnglishTextPreprocessor\n",
    "    print(\"âœ… EnglishTextPreprocessor importado\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ EnglishTextPreprocessor: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Teste 3: Criar processador (sem phonemes primeiro)\n",
    "try:\n",
    "    processor_basic = EnglishTextPreprocessor(\n",
    "        language=\"en-us\",\n",
    "        use_phonemes=False,  # Desabilitar phonemes para teste bÃ¡sico\n",
    "        normalize_numbers=True\n",
    "    )\n",
    "    print(\"âœ… Processador bÃ¡sico criado\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Processador bÃ¡sico: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Teste 4: NormalizaÃ§Ã£o bÃ¡sica\n",
    "try:\n",
    "    test_text = \"Hello world! This is a test with numbers 123 and Mr. Smith.\"\n",
    "    normalized = processor_basic.normalize_text(test_text)\n",
    "    print(f\"âœ… Texto normalizado: {normalized}\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ NormalizaÃ§Ã£o: {e}\")\n",
    "\n",
    "# Teste 5: Processador com phonemes (opcional)\n",
    "try:\n",
    "    processor_phonemes = EnglishTextPreprocessor(\n",
    "        language=\"en-us\",\n",
    "        use_phonemes=True,\n",
    "        normalize_numbers=True\n",
    "    )\n",
    "    print(\"âœ… Processador com phonemes criado\")\n",
    "\n",
    "    # Teste phonemes\n",
    "    try:\n",
    "        phonemes = processor_phonemes.text_to_phonemes(\"hello world\")\n",
    "        print(f\"âœ… Phonemes: {phonemes}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Phonemes: {e} - Funcionalidade opcional\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ Processador com phonemes: {e} - Funcionalidade opcional\")\n",
    "\n",
    "print(\"\\\\nâœ… Teste bÃ¡sico do processador concluÃ­do!\")\n",
    "print(\"ğŸ’¡ O sistema estÃ¡ pronto para treinamento mesmo se phonemes falharam\")\n",
    "'''\n",
    "\n",
    "# Executar teste robusto\n",
    "try:\n",
    "    with open(\"test_english_robust.py\", \"w\") as f:\n",
    "        f.write(test_code_robust)\n",
    "    run_command(\"python test_english_robust.py\", \"Teste robusto do processador inglÃªs\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Teste falhou: {e}\")\n",
    "    print(\"ğŸ’¡ Pode haver problemas com as dependÃªncias\")\n",
    "finally:\n",
    "    # Limpar arquivo de teste\n",
    "    if os.path.exists(\"test_english_robust.py\"):\n",
    "        os.remove(\"test_english_robust.py\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ… INSTALAÃ‡ÃƒO CONCLUÃDA!\")\n",
    "print(\"=\"*60)\n",
    "print(\"ğŸ¯ Sistema configurado para treinamento VITS2 em inglÃªs\")\n",
    "print(\"\")\n",
    "print(\"ğŸ“‹ Componentes instalados:\")\n",
    "print(\"   âœ… ESpeak (sistema) - para G2P em inglÃªs\")\n",
    "print(\"   âœ… Phonemizer 3.3.0 - conversÃ£o textoâ†’phonema\")\n",
    "print(\"   âœ… num2words - nÃºmeros em inglÃªs\")\n",
    "print(\"   âœ… inflect - pluralizaÃ§Ã£o em inglÃªs\")\n",
    "print(\"   âœ… ValeTTS - framework principal\")\n",
    "print(\"   âœ… EnglishTextPreprocessor - processamento especÃ­fico\")\n",
    "print(\"\")\n",
    "print(\"ğŸ’¡ NOTAS IMPORTANTES:\")\n",
    "print(\"   - Se phonemes falharam, o treinamento ainda funciona\")\n",
    "print(\"   - O sistema usarÃ¡ fallback para processamento de texto\")\n",
    "print(\"   - A performance pode ser ligeiramente reduzida mas funcional\")\n",
    "print(\"\")\n",
    "print(\"ğŸš€ PRÃ“XIMO PASSO: Execute a cÃ©lula de download do dataset!\")\n",
    "\n",
    "# VerificaÃ§Ã£o final dos imports crÃ­ticos\n",
    "print(\"\\nğŸ” VerificaÃ§Ã£o final...\")\n",
    "try:\n",
    "    import valetts\n",
    "    from valetts.data.preprocessing.text_en import EnglishTextPreprocessor\n",
    "    from valetts.models.vits2.model import VITS2\n",
    "    from valetts.models.vits2.config import VITS2Config\n",
    "    print(\"âœ… Todos os imports crÃ­ticos funcionando!\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Problema crÃ­tico: {e}\")\n",
    "    print(\"ğŸ”„ Pode ser necessÃ¡rio reiniciar o runtime e tentar novamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === DOWNLOAD E VERIFICAÃ‡ÃƒO COMPLETA DO DATASET ===\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"ğŸ“¥ INICIANDO VERIFICAÃ‡ÃƒO COMPLETA DO DATASET...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ConfiguraÃ§Ã£o de caminhos\n",
    "dataset_path = \"data/generated/Dataset-Unificado\"\n",
    "metadata_file = f\"{dataset_path}/metadata.csv\"\n",
    "audio_dir = f\"{dataset_path}/audio\"\n",
    "audio_raw_dir = f\"{dataset_path}/audio/raw\"\n",
    "\n",
    "print(f\"ğŸ“ Caminhos configurados:\")\n",
    "print(f\"   Dataset: {dataset_path}\")\n",
    "print(f\"   Metadata: {metadata_file}\")\n",
    "print(f\"   Ãudio: {audio_raw_dir}\")\n",
    "\n",
    "# ETAPA 1: Verificar se dataset existe localmente\n",
    "print(f\"\\nğŸ” ETAPA 1: Verificando existÃªncia local...\")\n",
    "if os.path.exists(metadata_file):\n",
    "    print(\"âœ… Dataset encontrado localmente\")\n",
    "    print(f\"   ğŸ“„ Metadata: {metadata_file}\")\n",
    "    print(f\"   ğŸ“Š Tamanho: {os.path.getsize(metadata_file):,} bytes\")\n",
    "else:\n",
    "    print(\"ğŸ“¥ Dataset nÃ£o encontrado localmente\")\n",
    "\n",
    "    # ETAPA 2: Verificar Google Drive\n",
    "    print(f\"\\nğŸ” ETAPA 2: Verificando Google Drive...\")\n",
    "    drive_dataset = \"/content/drive/MyDrive/ValeTTS-Colab/data/generated/Dataset-Unificado\"\n",
    "\n",
    "    if os.path.exists(drive_dataset):\n",
    "        print(f\"âœ… Dataset encontrado no Drive!\")\n",
    "        print(f\"   ğŸ“ Local: {drive_dataset}\")\n",
    "\n",
    "        # Mostrar conteÃºdo do Drive\n",
    "        drive_contents = os.listdir(drive_dataset)\n",
    "        print(f\"   ğŸ“‹ ConteÃºdo: {drive_contents}\")\n",
    "\n",
    "        print(f\"\\nğŸ”— Criando estrutura local...\")\n",
    "        os.makedirs(\"data/generated\", exist_ok=True)\n",
    "\n",
    "        # Verificar se jÃ¡ existe link/diretÃ³rio\n",
    "        if os.path.exists(dataset_path):\n",
    "            if os.path.islink(dataset_path):\n",
    "                print(\"   ğŸ—‘ï¸ Removendo link simbÃ³lico anterior...\")\n",
    "                os.unlink(dataset_path)\n",
    "            elif os.path.isdir(dataset_path):\n",
    "                print(\"   âš ï¸ DiretÃ³rio jÃ¡ existe, usando existente\")\n",
    "\n",
    "        if not os.path.exists(dataset_path):\n",
    "            print(\"   ğŸ”— Criando link simbÃ³lico...\")\n",
    "            os.symlink(drive_dataset, dataset_path)\n",
    "            print(\"   âœ… Link criado com sucesso!\")\n",
    "    else:\n",
    "        print(\"âŒ Dataset nÃ£o encontrado no Drive\")\n",
    "        print(f\"   ğŸ“ Procurado em: {drive_dataset}\")\n",
    "\n",
    "        print(f\"\\nğŸ’¡ SOLUÃ‡Ã•ES PARA OBTER O DATASET:\")\n",
    "        print(f\"\")\n",
    "        print(f\"ğŸ”§ MÃ‰TODO 1 - Google Drive (RECOMENDADO):\")\n",
    "        print(f\"   1. FaÃ§a upload do dataset para:\")\n",
    "        print(f\"      {drive_dataset}\")\n",
    "        print(f\"   2. Execute esta cÃ©lula novamente\")\n",
    "        print(f\"\")\n",
    "        print(f\"ğŸ”§ MÃ‰TODO 2 - Upload direto:\")\n",
    "        print(f\"   1. Use o Ã­cone ğŸ“ no painel esquerdo\")\n",
    "        print(f\"   2. Carregue para: {dataset_path}\")\n",
    "        print(f\"\")\n",
    "        print(f\"ğŸ”§ MÃ‰TODO 3 - Download automÃ¡tico:\")\n",
    "        print(f\"   Execute a cÃ©lula 4.5 (Download AutomÃ¡tico) primeiro!\")\n",
    "\n",
    "        # Parar aqui se nÃ£o encontrou o dataset\n",
    "        print(f\"\\nâ›” PARANDO: Dataset nÃ£o encontrado!\")\n",
    "        print(f\"ğŸ“‹ Execute uma das soluÃ§Ãµes acima primeiro.\")\n",
    "\n",
    "# ETAPA 3: VerificaÃ§Ã£o detalhada do dataset\n",
    "if os.path.exists(metadata_file):\n",
    "    print(f\"\\nğŸ” ETAPA 3: Analisando estrutura do dataset...\")\n",
    "\n",
    "    # Verificar estrutura de diretÃ³rios\n",
    "    print(f\"ğŸ“ Verificando estrutura:\")\n",
    "    structure_checks = [\n",
    "        (dataset_path, \"Dataset base\"),\n",
    "        (audio_dir, \"Pasta audio\"),\n",
    "        (audio_raw_dir, \"Pasta audio/raw\"),\n",
    "        (metadata_file, \"Arquivo metadata.csv\")\n",
    "    ]\n",
    "\n",
    "    for path, description in structure_checks:\n",
    "        exists = os.path.exists(path)\n",
    "        if exists:\n",
    "            if os.path.isfile(path):\n",
    "                size = os.path.getsize(path)\n",
    "                print(f\"   âœ… {description}: {size:,} bytes\")\n",
    "            else:\n",
    "                items = len(os.listdir(path))\n",
    "                print(f\"   âœ… {description}: {items:,} itens\")\n",
    "        else:\n",
    "            print(f\"   âŒ {description}: NÃƒO ENCONTRADO\")\n",
    "\n",
    "    # ETAPA 4: AnÃ¡lise do metadata.csv\n",
    "    print(f\"\\nğŸ” ETAPA 4: Carregando e analisando metadata...\")\n",
    "    try:\n",
    "        print(f\"ğŸ“Š Carregando CSV...\")\n",
    "        df = pd.read_csv(metadata_file)\n",
    "\n",
    "        total_samples = len(df)\n",
    "        unique_speakers = df['speaker_id'].nunique()\n",
    "        locales = df['locale'].unique() if 'locale' in df.columns else ['unknown']\n",
    "\n",
    "        print(f\"âœ… Metadata carregado com sucesso!\")\n",
    "        print(f\"   ğŸ“ˆ Total de amostras: {total_samples:,}\")\n",
    "        print(f\"   ğŸ¤ Falantes Ãºnicos: {unique_speakers}\")\n",
    "        print(f\"   ğŸŒ Idiomas: {locales}\")\n",
    "        print(f\"   ğŸ“‹ Colunas: {list(df.columns)}\")\n",
    "\n",
    "        # Verificar idioma inglÃªs\n",
    "        if 'locale' in df.columns:\n",
    "            english_samples = len(df[df['locale'] == 'en']) if 'en' in locales else 0\n",
    "            print(f\"   ğŸ‡ºğŸ‡¸ Amostras em inglÃªs: {english_samples:,}\")\n",
    "\n",
    "            if english_samples == 0:\n",
    "                print(\"   âŒ PROBLEMA: Nenhuma amostra em inglÃªs!\")\n",
    "            elif english_samples < 1000:\n",
    "                print(\"   âš ï¸ AVISO: Poucas amostras para treinamento robusto\")\n",
    "            else:\n",
    "                print(\"   âœ… Quantidade adequada para treinamento\")\n",
    "\n",
    "        # Verificar colunas obrigatÃ³rias\n",
    "        required_columns = ['audio_path', 'text_normalized', 'speaker_id']\n",
    "        missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "\n",
    "        if missing_columns:\n",
    "            print(f\"   âŒ PROBLEMA: Colunas ausentes: {missing_columns}\")\n",
    "        else:\n",
    "            print(f\"   âœ… Todas as colunas obrigatÃ³rias presentes\")\n",
    "\n",
    "        # ETAPA 5: VerificaÃ§Ã£o dos arquivos de Ã¡udio\n",
    "        print(f\"\\nğŸ” ETAPA 5: Verificando arquivos de Ã¡udio...\")\n",
    "\n",
    "        # Inicializar variÃ¡veis\n",
    "        total_wav_files = 0\n",
    "        found_count = 0\n",
    "\n",
    "        # Contar arquivos reais\n",
    "        if os.path.exists(audio_raw_dir):\n",
    "            wav_files = [f for f in os.listdir(audio_raw_dir) if f.endswith('.wav')]\n",
    "            total_wav_files = len(wav_files)\n",
    "            print(f\"ğŸ“Š Arquivos WAV encontrados: {total_wav_files:,}\")\n",
    "\n",
    "            # Verificar correspondÃªncia\n",
    "            csv_entries = len(df)\n",
    "            if total_wav_files == csv_entries:\n",
    "                print(f\"âœ… PERFEITO: NÃºmero de arquivos corresponde ao CSV!\")\n",
    "            elif total_wav_files == csv_entries - 1:  # Considerando header\n",
    "                print(f\"âœ… BOM: Arquivos correspondem (considerando header do CSV)\")\n",
    "            else:\n",
    "                print(f\"âš ï¸ DIVERGÃŠNCIA: CSV={csv_entries}, WAV={total_wav_files}\")\n",
    "\n",
    "            # Verificar primeiros arquivos\n",
    "            print(f\"\\nğŸ” Testando primeiros 10 arquivos...\")\n",
    "            sample_size = min(10, len(df))\n",
    "\n",
    "            for i in range(sample_size):\n",
    "                audio_path_csv = df.iloc[i]['audio_path']\n",
    "                full_path = os.path.join(dataset_path, audio_path_csv)\n",
    "\n",
    "                if os.path.exists(full_path):\n",
    "                    size = os.path.getsize(full_path)\n",
    "                    found_count += 1\n",
    "                    print(f\"   âœ… {audio_path_csv} ({size:,} bytes)\")\n",
    "                else:\n",
    "                    print(f\"   âŒ {audio_path_csv} - NÃƒO ENCONTRADO\")\n",
    "\n",
    "            print(f\"\\nğŸ“Š Resultado da amostra: {found_count}/{sample_size} arquivos encontrados\")\n",
    "\n",
    "            # Verificar tamanhos dos arquivos\n",
    "            if found_count > 0:\n",
    "                print(f\"\\nğŸ“Š EstatÃ­sticas dos arquivos:\")\n",
    "                sizes = []\n",
    "                for i in range(min(100, len(df))):  # Amostra de 100 arquivos\n",
    "                    audio_path_csv = df.iloc[i]['audio_path']\n",
    "                    full_path = os.path.join(dataset_path, audio_path_csv)\n",
    "                    if os.path.exists(full_path):\n",
    "                        sizes.append(os.path.getsize(full_path))\n",
    "\n",
    "                if sizes:\n",
    "                    avg_size = sum(sizes) / len(sizes)\n",
    "                    min_size = min(sizes)\n",
    "                    max_size = max(sizes)\n",
    "                    print(f\"   ğŸ“ Tamanho mÃ©dio: {avg_size:,.0f} bytes\")\n",
    "                    print(f\"   ğŸ“ Menor arquivo: {min_size:,} bytes\")\n",
    "                    print(f\"   ğŸ“ Maior arquivo: {max_size:,} bytes\")\n",
    "\n",
    "            # Verificar formato dos arquivos\n",
    "            print(f\"\\nğŸ” Verificando formato do primeiro arquivo...\")\n",
    "            if found_count > 0:\n",
    "                first_audio = os.path.join(dataset_path, df.iloc[0]['audio_path'])\n",
    "                import subprocess\n",
    "                try:\n",
    "                    result = subprocess.run(['file', first_audio], capture_output=True, text=True)\n",
    "                    print(f\"   ğŸµ Formato: {result.stdout.strip()}\")\n",
    "                except:\n",
    "                    print(f\"   âš ï¸ NÃ£o foi possÃ­vel verificar o formato\")\n",
    "\n",
    "        else:\n",
    "            print(f\"âŒ Pasta de Ã¡udio nÃ£o encontrada: {audio_raw_dir}\")\n",
    "            print(f\"ğŸ’¡ Os arquivos de Ã¡udio precisam ser baixados!\")\n",
    "            print(f\"\")\n",
    "            print(f\"ğŸ”§ SOLUÃ‡Ã•ES:\")\n",
    "            print(f\"1. Execute a cÃ©lula 4.5 (Download AutomÃ¡tico)\")\n",
    "            print(f\"2. Ou copie os arquivos manualmente para: {audio_raw_dir}\")\n",
    "\n",
    "        # ETAPA 6: Resumo final\n",
    "        print(f\"\\n\" + \"=\"*60)\n",
    "        print(f\"ğŸ“Š RESUMO FINAL DO DATASET:\")\n",
    "        print(f\"=\"*60)\n",
    "        print(f\"âœ… Metadata: {total_samples:,} amostras\")\n",
    "        print(f\"âœ… Falantes: {unique_speakers}\")\n",
    "        print(f\"âœ… Idioma: inglÃªs ({english_samples:,} amostras)\")\n",
    "        print(f\"{'âœ…' if total_wav_files > 0 else 'âŒ'} Arquivos WAV: {total_wav_files:,}\")\n",
    "        print(f\"{'âœ…' if found_count == sample_size else 'âš ï¸'} CorrespondÃªncia: {found_count}/{sample_size} testados\")\n",
    "\n",
    "        # Status final\n",
    "        all_good = (\n",
    "            english_samples > 0 and\n",
    "            not missing_columns and\n",
    "            total_wav_files > 0 and\n",
    "            found_count >= sample_size // 2  # Pelo menos 50% dos arquivos de teste\n",
    "        )\n",
    "\n",
    "        if all_good:\n",
    "            print(f\"\\nğŸš€ DATASET COMPLETAMENTE PRONTO!\")\n",
    "            print(f\"âœ… Pode prosseguir para o treinamento\")\n",
    "            print(f\"ğŸ“Š Performance esperada: excelente com {english_samples:,} amostras\")\n",
    "        elif total_wav_files == 0:\n",
    "            print(f\"\\nâŒ DATASET INCOMPLETO - FALTAM ARQUIVOS DE ÃUDIO\")\n",
    "            print(f\"ğŸ”§ Execute a cÃ©lula 4.5 (Download AutomÃ¡tico) primeiro!\")\n",
    "            print(f\"ğŸ“¦ SÃ³ temos o metadata, precisamos dos arquivos WAV\")\n",
    "        else:\n",
    "            print(f\"\\nâš ï¸ DATASET TEM PROBLEMAS MENORES\")\n",
    "            print(f\"ğŸ”§ Verifique os pontos marcados acima\")\n",
    "            print(f\"ğŸ’¡ O treinamento pode ainda assim funcionar\")\n",
    "\n",
    "        # Mostrar amostra dos dados\n",
    "        print(f\"\\nğŸ“‹ Amostra dos dados:\")\n",
    "        sample_cols = ['speaker_id', 'text_normalized', 'locale']\n",
    "        available_cols = [col for col in sample_cols if col in df.columns]\n",
    "        print(df.head(3)[available_cols].to_string())\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ERRO ao analisar metadata: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "else:\n",
    "    print(f\"\\nâŒ DATASET NÃƒO DISPONÃVEL\")\n",
    "    print(f\"âš ï¸ Execute os mÃ©todos de download mostrados acima\")\n",
    "\n",
    "print(f\"\\nğŸ“ STATUS FINAL:\")\n",
    "print(f\"   Dataset: {'âœ…' if os.path.exists(dataset_path) else 'âŒ'}\")\n",
    "print(f\"   Metadata: {'âœ…' if os.path.exists(metadata_file) else 'âŒ'}\")\n",
    "print(f\"   Ãudio: {'âœ…' if os.path.exists(audio_raw_dir) and total_wav_files > 0 else 'âŒ'}\")\n",
    "\n",
    "# Determinar se estÃ¡ pronto baseado na presenÃ§a de metadata e Ã¡udio\n",
    "metadata_ready = os.path.exists(metadata_file)\n",
    "audio_ready = os.path.exists(audio_raw_dir) and total_wav_files > 0\n",
    "dataset_ready = metadata_ready and audio_ready\n",
    "\n",
    "print(f\"   Pronto: {'âœ… SIM' if dataset_ready else 'âŒ NÃƒO'}\")\n",
    "\n",
    "if dataset_ready:\n",
    "    print(f\"\\nğŸ¯ PRÃ“XIMO PASSO: Execute a cÃ©lula de configuraÃ§Ã£o do modelo!\")\n",
    "elif metadata_ready and not audio_ready:\n",
    "    print(f\"\\nğŸ¯ PRÃ“XIMO PASSO: Execute a cÃ©lula 4.5 (Download AutomÃ¡tico) para obter os arquivos de Ã¡udio!\")\n",
    "else:\n",
    "    print(f\"\\nğŸ¯ PRÃ“XIMO PASSO: Configure o dataset primeiro!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CONFIGURAÃ‡ÃƒO DINÃ‚MICA DO MODELO PARA INGLÃŠS ===\n",
    "import yaml\n",
    "import pandas as pd\n",
    "\n",
    "# Primeiro, verificar o dataset\n",
    "metadata_file = \"data/generated/Dataset-Unificado/metadata.csv\"\n",
    "if os.path.exists(metadata_file):\n",
    "    df = pd.read_csv(metadata_file)\n",
    "    total_samples = len(df)\n",
    "    unique_speakers = df['speaker_id'].nunique()\n",
    "    locales = df['locale'].unique()\n",
    "\n",
    "    print(f\"ğŸ“Š Dataset carregado:\")\n",
    "    print(f\"   ğŸ“ˆ Total: {total_samples:,} amostras\")\n",
    "    print(f\"   ğŸ¤ Falantes: {unique_speakers}\")\n",
    "    print(f\"   ğŸŒ Idiomas: {locales}\")\n",
    "\n",
    "    # Verificar se Ã© inglÃªs\n",
    "    if 'en' not in locales:\n",
    "        print(\"âš ï¸ AVISO: Dataset nÃ£o contÃ©m inglÃªs!\")\n",
    "        print(f\"   Idiomas encontrados: {locales}\")\n",
    "    else:\n",
    "        english_samples = len(df[df['locale'] == 'en'])\n",
    "        print(f\"   âœ… Amostras em inglÃªs: {english_samples:,}\")\n",
    "else:\n",
    "    print(\"âŒ Metadata nÃ£o encontrada!\")\n",
    "    total_samples = 22910  # Fallback\n",
    "    unique_speakers = 52\n",
    "\n",
    "# Usar configuraÃ§Ã£o especÃ­fica para A100 se disponÃ­vel\n",
    "if CONFIG_NAME == \"vits2_english_a100_optimized\":\n",
    "    # Carregar configuraÃ§Ã£o A100 otimizada\n",
    "    config_path = f\"configs/training/{CONFIG_NAME}.yaml\"\n",
    "    if os.path.exists(config_path):\n",
    "        with open(config_path, 'r') as f:\n",
    "            config = yaml.safe_load(f)\n",
    "        print(\"âœ… Usando configuraÃ§Ã£o A100 otimizada!\")\n",
    "    else:\n",
    "        print(\"âš ï¸ ConfiguraÃ§Ã£o A100 nÃ£o encontrada, criando...\")\n",
    "        config = {}\n",
    "else:\n",
    "    config = {}\n",
    "\n",
    "# ConfiguraÃ§Ã£o base ou atualizaÃ§Ã£o\n",
    "if not config or 'model' not in config:\n",
    "    if DEBUG_MODE:\n",
    "        model_config = {\n",
    "            \"text_encoder_hidden_dim\": 128,\n",
    "            \"latent_dim\": 128,\n",
    "            \"speaker_embedding_dim\": 256,\n",
    "            \"generator_initial_channels\": 256,\n",
    "            \"decoder_hidden_dim\": 256,\n",
    "        }\n",
    "    else:\n",
    "        # ConfiguraÃ§Ã£o baseada na GPU\n",
    "        if \"A100\" in gpu_name:\n",
    "            model_config = {\n",
    "                \"text_encoder_hidden_dim\": 512,\n",
    "                \"latent_dim\": 512,\n",
    "                \"speaker_embedding_dim\": 1024,\n",
    "                \"generator_initial_channels\": 1024,\n",
    "                \"decoder_hidden_dim\": 1024,\n",
    "            }\n",
    "        else:\n",
    "            model_config = {\n",
    "                \"text_encoder_hidden_dim\": 192,\n",
    "                \"latent_dim\": 192,\n",
    "                \"speaker_embedding_dim\": 512,\n",
    "                \"generator_initial_channels\": 512,\n",
    "                \"decoder_hidden_dim\": 512,\n",
    "            }\n",
    "\n",
    "    config = {\n",
    "        \"model\": {\n",
    "            \"name\": \"VITS2\",\n",
    "            \"mel_channels\": 80,\n",
    "            \"n_speakers\": unique_speakers,\n",
    "            \"text_processor\": \"english\",\n",
    "            \"inference_only\": False,\n",
    "            **model_config\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Atualizar configuraÃ§Ãµes de treinamento\n",
    "config.update({\n",
    "    \"training\": {\n",
    "        \"learning_rate\": 2.0e-4,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"max_epochs\": EPOCHS,\n",
    "        \"accumulate_grad_batches\": 1,\n",
    "        \"max_grad_norm\": 1.0,\n",
    "        \"mel_loss_weight\": 45.0,\n",
    "        \"kl_loss_weight\": 1.0,\n",
    "        \"adv_loss_weight\": 1.0,\n",
    "        \"fm_loss_weight\": 2.0,\n",
    "        \"duration_loss_weight\": 1.0,\n",
    "        \"use_amp\": True,\n",
    "        \"gradient_clip_val\": 1.0,\n",
    "        \"discriminator_update_frequency\": 1,\n",
    "        \"scheduler\": {\n",
    "            \"name\": \"ReduceLROnPlateau\",\n",
    "            \"mode\": \"min\",\n",
    "            \"factor\": 0.5,\n",
    "            \"patience\": 15 if not DEBUG_MODE else 5,\n",
    "            \"min_lr\": 1.0e-6\n",
    "        }\n",
    "    },\n",
    "    \"data\": {\n",
    "        \"dataset_format\": \"valetts\",\n",
    "        \"data_dir\": \"data/generated/Dataset-Unificado\",\n",
    "        \"metadata_file\": \"data/generated/Dataset-Unificado/metadata.csv\",\n",
    "        \"language\": \"en\",  # CRÃTICO: usar \"en\" para inglÃªs\n",
    "        \"locale_column\": \"locale\",\n",
    "        \"text_processor\": {\n",
    "            \"class\": \"EnglishTextPreprocessor\",\n",
    "            \"use_phonemes\": True,\n",
    "            \"normalize_numbers\": True,\n",
    "            \"normalize_whitespace\": True,\n",
    "            \"lowercase\": True,\n",
    "            \"backend\": \"espeak\",\n",
    "            \"language\": \"en-us\"\n",
    "        },\n",
    "        \"sample_rate\": 22050,\n",
    "        \"n_mels\": 80,\n",
    "        \"n_fft\": 1024,\n",
    "        \"hop_length\": 256,\n",
    "        \"win_length\": 1024,\n",
    "        \"num_workers\": 8 if \"A100\" in gpu_name else 4,\n",
    "        \"pin_memory\": True,\n",
    "        \"persistent_workers\": True,\n",
    "        \"use_augmentation\": True,\n",
    "        \"volume_range\": [0.9, 1.1],\n",
    "        \"pitch_range\": [-1, 1]\n",
    "    },\n",
    "    \"logging\": {\n",
    "        \"log_dir\": \"logs\",\n",
    "        \"experiment_name\": f\"vits2_english_{CONFIG_NAME.split('_')[-1]}\",\n",
    "\n",
    "        # SISTEMA HÃBRIDO DE CHECKPOINTS\n",
    "        \"checkpoint\": {\n",
    "            # Checkpoints principais (2 Ãºltimos - se sobrescrevem)\n",
    "            \"dirpath\": f\"checkpoints/vits2_english_{CONFIG_NAME.split('_')[-1]}\",\n",
    "            \"filename\": f\"vits2_english-{{epoch:03d}}-{{epoch/val_loss_total:.3f}}\",\n",
    "            \"monitor\": \"epoch/val_loss_total\",\n",
    "            \"mode\": \"min\",\n",
    "            \"save_top_k\": 2,  # Apenas 2 checkpoints recentes\n",
    "            \"save_last\": True,\n",
    "            \"every_n_epochs\": 1,  # Salvar a cada Ã©poca\n",
    "            \"auto_insert_metric_name\": False,\n",
    "            \"save_on_train_epoch_end\": True\n",
    "        },\n",
    "\n",
    "        # Checkpoints de backup (a cada 10 Ã©pocas - permanentes)\n",
    "        \"checkpoint_backup\": {\n",
    "            \"enabled\": True,\n",
    "            \"dirpath\": f\"checkpoints/vits2_english_{CONFIG_NAME.split('_')[-1]}/backup\",\n",
    "            \"filename\": f\"vits2_english_backup-{{epoch:03d}}-{{epoch/val_loss_total:.3f}}\",\n",
    "            \"every_n_epochs\": 10,  # Backup a cada 10 Ã©pocas\n",
    "            \"save_top_k\": -1,  # Salvar todos os backups (nunca sobrescrever)\n",
    "            \"monitor\": \"epoch/val_loss_total\",\n",
    "            \"mode\": \"min\"\n",
    "        },\n",
    "\n",
    "        \"early_stopping\": {\n",
    "            \"monitor\": \"epoch/val_loss_total\",\n",
    "            \"mode\": \"min\",\n",
    "            \"patience\": 10 if DEBUG_MODE else 30,\n",
    "            \"min_delta\": 0.001\n",
    "        },\n",
    "\n",
    "        \"tensorboard\": {\n",
    "            \"save_dir\": \"logs/tensorboard\",\n",
    "            \"name\": f\"vits2_english_{CONFIG_NAME.split('_')[-1]}\"\n",
    "        }\n",
    "    },\n",
    "    \"hardware\": {\n",
    "        \"accelerator\": \"gpu\",\n",
    "        \"devices\": 1,\n",
    "        \"precision\": \"16-mixed\",\n",
    "        \"strategy\": \"auto\",\n",
    "        \"benchmark\": True if \"A100\" in gpu_name else False\n",
    "    },\n",
    "    \"validation\": {\n",
    "        \"val_check_interval\": 1.0,\n",
    "        \"generate_samples\": True,\n",
    "        \"sample_every_n_epochs\": 1,  # Gerar amostras a cada Ã©poca\n",
    "        \"limit_val_batches\": 1.0,\n",
    "        \"num_sanity_val_steps\": 2\n",
    "    },\n",
    "    \"dataset_config\": {\n",
    "        \"expected_locale\": \"en\",  # CRÃTICO: valor correto para inglÃªs\n",
    "        \"validate_files\": True,\n",
    "        \"cache_preprocessing\": True,\n",
    "        \"audio_column\": \"audio_path\",\n",
    "        \"text_column\": \"text_normalized\",\n",
    "        \"speaker_column\": \"speaker_id\",\n",
    "        \"locale_column\": \"locale\"\n",
    "    },\n",
    "\n",
    "    # CONFIGURAÃ‡ÃƒO LLM INTEGRADA\n",
    "    \"llm_monitor\": LLM_CONFIG,\n",
    "\n",
    "    # CONFIGURAÃ‡ÃƒO DE AMOSTRAS DE ÃUDIO\n",
    "    \"audio_sampling\": {\n",
    "        \"enabled\": True,\n",
    "        \"sample_every_n_epochs\": 1,  # Gerar amostras a cada Ã©poca\n",
    "        \"num_samples\": 3,  # 3 amostras por checkpoint\n",
    "        \"max_length\": 10.0,  # MÃ¡ximo 10 segundos\n",
    "        \"sample_rate\": 22050,\n",
    "        \"save_dir\": \"samples\",\n",
    "        \"speakers_to_sample\": [0, 1, 2]  # Primeiros 3 speakers\n",
    "    }\n",
    "})\n",
    "\n",
    "# Adicionar limitaÃ§Ã£o de amostras para debug\n",
    "if DEBUG_MODE:\n",
    "    config[\"data\"][\"max_samples_debug\"] = MAX_SAMPLES\n",
    "\n",
    "# Criar diretÃ³rios necessÃ¡rios\n",
    "os.makedirs(\"configs/training\", exist_ok=True)\n",
    "os.makedirs(config[\"logging\"][\"checkpoint\"][\"dirpath\"], exist_ok=True)\n",
    "os.makedirs(config[\"logging\"][\"checkpoint_backup\"][\"dirpath\"], exist_ok=True)  # DiretÃ³rio backup\n",
    "os.makedirs(\"logs/tensorboard\", exist_ok=True)\n",
    "os.makedirs(\"samples\", exist_ok=True)\n",
    "\n",
    "# Salvar configuraÃ§Ã£o\n",
    "config_path = f\"configs/training/{CONFIG_NAME}.yaml\"\n",
    "with open(config_path, 'w') as f:\n",
    "    yaml.dump(config, f, default_flow_style=False, indent=2)\n",
    "\n",
    "print(f\"âœ… ConfiguraÃ§Ã£o criada: {config_path}\")\n",
    "print(f\"ğŸ¯ Modo: {CONFIG_NAME.upper()}\")\n",
    "print(f\"ğŸ“Š Ã‰pocas: {EPOCHS}\")\n",
    "print(f\"ğŸ“¦ Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"ğŸ¤ Falantes: {config['model']['n_speakers']}\")\n",
    "print(f\"ğŸŒ Idioma: INGLÃŠS (locale='en')\")\n",
    "print(f\"ğŸ¤– Monitor LLM: {'âœ… Ativo' if LLM_CONFIG['enabled'] else 'âŒ Desabilitado'}\")\n",
    "\n",
    "print(f\"\\nğŸ’¾ Sistema HÃ­brido de Checkpoints:\")\n",
    "print(f\"   ğŸ“¦ Checkpoints recentes: 2 (se sobrescrevem a cada Ã©poca)\")\n",
    "print(f\"   ğŸ”’ Checkpoints backup: A cada 10 Ã©pocas (permanentes)\")\n",
    "print(f\"   ğŸ“ DiretÃ³rio principal: {config['logging']['checkpoint']['dirpath']}\")\n",
    "print(f\"   ğŸ—„ï¸ DiretÃ³rio backup: {config['logging']['checkpoint_backup']['dirpath']}\")\n",
    "print(f\"   ğŸµ Amostras de Ã¡udio: A cada Ã©poca\")\n",
    "\n",
    "# Estimar espaÃ§o em disco\n",
    "epochs_to_run = EPOCHS\n",
    "recent_checkpoints_size = 2 * 150  # 2 checkpoints Ã— ~150MB cada\n",
    "backup_checkpoints_count = (epochs_to_run // 10) + 1\n",
    "backup_checkpoints_size = backup_checkpoints_count * 150\n",
    "total_checkpoints_size = recent_checkpoints_size + backup_checkpoints_size\n",
    "\n",
    "print(f\"\\nğŸ’¾ Estimativa de EspaÃ§o em Disco:\")\n",
    "print(f\"   ğŸ“¦ Checkpoints recentes: ~{recent_checkpoints_size}MB\")\n",
    "print(f\"   ğŸ”’ Checkpoints backup: ~{backup_checkpoints_size}MB ({backup_checkpoints_count} arquivos)\")\n",
    "print(f\"   ğŸµ Amostras de Ã¡udio: ~{epochs_to_run * 3 * 0.03:.1f}MB\")\n",
    "print(f\"   ğŸ“Š Total estimado: ~{total_checkpoints_size + (epochs_to_run * 3 * 0.03):.0f}MB\")\n",
    "\n",
    "print(f\"\\nğŸ’¾ DimensÃµes do modelo:\")\n",
    "print(f\"   - Hidden: {config['model']['text_encoder_hidden_dim']}\")\n",
    "print(f\"   - Speaker: {config['model']['speaker_embedding_dim']}\")\n",
    "print(f\"   - Generator: {config['model']['generator_initial_channels']}\")\n",
    "print(f\"   - Decoder: {config['model']['decoder_hidden_dim']}\")\n",
    "\n",
    "# Verificar compatibilidade de dimensÃµes CRÃTICA\n",
    "if config['model']['speaker_embedding_dim'] == config['model']['decoder_hidden_dim']:\n",
    "    print(\"âœ… DimensÃµes compatÃ­veis - Sem erro de tensor!\")\n",
    "else:\n",
    "    print(\"âŒ AVISO: DimensÃµes incompatÃ­veis detectadas!\")\n",
    "    print(f\"   Speaker: {config['model']['speaker_embedding_dim']}\")\n",
    "    print(f\"   Decoder: {config['model']['decoder_hidden_dim']}\")\n",
    "\n",
    "print(f\"\\nğŸš€ ConfiguraÃ§Ã£o otimizada para {gpu_name} pronta!\")\n",
    "\n",
    "if LLM_CONFIG['enabled']:\n",
    "    print(\"\\nğŸ¤– Monitor LLM ativo - benefÃ­cios:\")\n",
    "    print(\"   ğŸ“Š AnÃ¡lise inteligente do progresso\")\n",
    "    print(\"   âš™ï¸ Ajustes automÃ¡ticos de hiperparÃ¢metros\")\n",
    "    print(\"   ğŸ¯ DetecÃ§Ã£o precoce de problemas\")\n",
    "    print(\"   ğŸ“ˆ RelatÃ³rios detalhados de treinamento\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === INICIALIZAÃ‡ÃƒO DO TENSORBOARD ===\n",
    "# Configurar TensorBoard antes do treinamento para monitoramento em tempo real\n",
    "\n",
    "# Verificar se as variÃ¡veis necessÃ¡rias estÃ£o definidas\n",
    "try:\n",
    "    config\n",
    "    CONFIG_NAME\n",
    "except NameError as e:\n",
    "    print(f\"âŒ ERRO: Execute a cÃ©lula de configuraÃ§Ã£o do modelo primeiro!\")\n",
    "    raise SystemExit(\"Execute as cÃ©lulas anteriores primeiro!\")\n",
    "\n",
    "print(\"ğŸ“Š Configurando TensorBoard para monitoramento em tempo real...\")\n",
    "\n",
    "# Configurar diretÃ³rio do TensorBoard\n",
    "tensorboard_dir = config[\"logging\"][\"tensorboard\"][\"save_dir\"]\n",
    "experiment_name = config[\"logging\"][\"tensorboard\"][\"name\"]\n",
    "\n",
    "print(f\"ğŸ“ DiretÃ³rio: {tensorboard_dir}\")\n",
    "print(f\"ğŸ¯ Experimento: {experiment_name}\")\n",
    "\n",
    "# Criar diretÃ³rio se nÃ£o existir\n",
    "os.makedirs(tensorboard_dir, exist_ok=True)\n",
    "\n",
    "# Inicializar TensorBoard no Colab\n",
    "try:\n",
    "    # Carregar extensÃ£o do TensorBoard\n",
    "    %load_ext tensorboard\n",
    "\n",
    "    # Iniciar TensorBoard\n",
    "    %tensorboard --logdir={tensorboard_dir} --reload_interval=30\n",
    "\n",
    "    print(\"âœ… TensorBoard iniciado com sucesso!\")\n",
    "    print(\"ğŸ“Š Acesse a aba 'TensorBoard' acima para ver os grÃ¡ficos em tempo real\")\n",
    "    print(f\"ğŸ”„ AtualizaÃ§Ã£o automÃ¡tica a cada 30 segundos\")\n",
    "    print(\"\\nğŸ“ˆ MÃ©tricas disponÃ­veis:\")\n",
    "    print(\"   - Loss total de treinamento e validaÃ§Ã£o\")\n",
    "    print(\"   - Loss MEL, KL, Adversarial, Feature Matching\")\n",
    "    print(\"   - Learning rate e gradient norm\")\n",
    "    print(\"   - Amostras de Ã¡udio geradas\")\n",
    "    print(\"   - GrÃ¡ficos de convergÃªncia\")\n",
    "\n",
    "    if LLM_CONFIG['enabled']:\n",
    "        print(\"   - AnÃ¡lises do Monitor LLM (a cada 10 Ã©pocas)\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ Erro ao inicializar TensorBoard: {e}\")\n",
    "    print(\"ğŸ’¡ TensorBoard serÃ¡ inicializado automaticamente durante o treinamento\")\n",
    "\n",
    "print(\"\\nğŸ¯ Sistema pronto para treinamento com monitoramento completo!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === INICIAR TREINAMENTO OTIMIZADO PARA INGLÃŠS ===\n",
    "\n",
    "# Verificar se as variÃ¡veis necessÃ¡rias estÃ£o definidas\n",
    "try:\n",
    "    CONFIG_NAME\n",
    "    config_path\n",
    "    gpu_name\n",
    "    gpu_memory\n",
    "    BATCH_SIZE\n",
    "    ESTIMATED_TIME\n",
    "    EXPECTED_PERFORMANCE\n",
    "    LLM_CONFIG\n",
    "except NameError as e:\n",
    "    print(f\"âŒ ERRO: VariÃ¡vel nÃ£o definida - {e}\")\n",
    "    print(\"ğŸ’¡ SOLUÃ‡ÃƒO: Execute as cÃ©lulas anteriores em ordem:\")\n",
    "    print(\"   1. ConfiguraÃ§Ã£o da API OpenRouter\")\n",
    "    print(\"   2. ConfiguraÃ§Ã£o do modo de treinamento\")\n",
    "    print(\"   3. Setup do sistema\")\n",
    "    print(\"   4. InstalaÃ§Ã£o de dependÃªncias\")\n",
    "    print(\"   5. Download do dataset\")\n",
    "    print(\"   6. ConfiguraÃ§Ã£o do modelo\")\n",
    "    print(\"   7. Esta cÃ©lula de treinamento\")\n",
    "    print(\"\")\n",
    "    print(\"ğŸ”„ Execute novamente a partir da cÃ©lula 2 (ConfiguraÃ§Ã£o do modo)\")\n",
    "    raise SystemExit(\"Execute as cÃ©lulas anteriores primeiro!\")\n",
    "\n",
    "print(f\"ğŸš€ Iniciando treinamento VITS2 - Modo: {CONFIG_NAME.upper()}\")\n",
    "print(f\"ğŸ“ ConfiguraÃ§Ã£o: {config_path}\")\n",
    "print(f\"ğŸ® GPU: {gpu_name} ({gpu_memory} MB)\")\n",
    "print(f\"ğŸ“¦ Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"â±ï¸ Estimativa: {ESTIMATED_TIME}\")\n",
    "print(f\"âš¡ Performance esperada: {EXPECTED_PERFORMANCE}\")\n",
    "print(f\"ğŸ¤– Monitor LLM: {'âœ… Ativo' if LLM_CONFIG['enabled'] else 'âŒ Desabilitado'}\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Verificar se temos script especÃ­fico para inglÃªs\n",
    "english_script = \"scripts/train_vits2_english.py\"\n",
    "if os.path.exists(english_script):\n",
    "    print(\"âœ… Usando script especÃ­fico para inglÃªs\")\n",
    "    base_cmd = f\"python {english_script} --config {config_path}\"\n",
    "else:\n",
    "    print(\"âš ï¸ Script especÃ­fico nÃ£o encontrado, usando script padrÃ£o\")\n",
    "    base_cmd = f\"python scripts/train_vits2.py --config {config_path}\"\n",
    "\n",
    "# Configurar comando baseado no LLM\n",
    "if LLM_CONFIG['enabled']:\n",
    "    print(\"ğŸ¤– Monitor LLM ativo - treinamento inteligente habilitado\")\n",
    "    cmd = base_cmd  # LLM jÃ¡ estÃ¡ na configuraÃ§Ã£o\n",
    "else:\n",
    "    print(\"ğŸ”§ Monitor LLM desabilitado - usando modo padrÃ£o\")\n",
    "    cmd = f\"{base_cmd} --disable-llm\"\n",
    "\n",
    "# Configurar CUDA para otimizaÃ§Ã£o mÃ¡xima\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '0'  # Async CUDA\n",
    "os.environ['TORCH_CUDNN_V8_API_ENABLED'] = '1'  # CuDNN v8\n",
    "os.environ['TORCH_ALLOW_TF32_CUBLAS_OVERRIDE'] = '1'  # TF32 para A100\n",
    "\n",
    "if \"A100\" in gpu_name:\n",
    "    print(\"ğŸš€ ConfiguraÃ§Ãµes especiais para A100:\")\n",
    "    print(\"   - TF32 habilitado para mÃ¡xima performance\")\n",
    "    print(\"   - CuDNN v8 API ativado\")\n",
    "    print(\"   - Tensor cores ativados automaticamente\")\n",
    "    os.environ['TORCH_ALLOW_TF32'] = '1'\n",
    "    os.environ['TORCH_CUDNN_ALLOW_TF32'] = '1'\n",
    "\n",
    "# ConfiguraÃ§Ãµes especÃ­ficas para Monitor LLM\n",
    "if LLM_CONFIG['enabled']:\n",
    "    print(\"\\nğŸ¤– ConfiguraÃ§Ãµes do Monitor LLM:\")\n",
    "    print(f\"   ğŸ§  Modelo: {LLM_CONFIG['model']}\")\n",
    "    print(f\"   ğŸ“Š AnÃ¡lise a cada: {LLM_CONFIG['monitor_every_epochs']} Ã©pocas\")\n",
    "    print(f\"   ğŸ”„ Provider: {LLM_CONFIG['provider']}\")\n",
    "    print(\"   ğŸ“ˆ Funcionalidades ativas:\")\n",
    "    print(\"      - OtimizaÃ§Ã£o automÃ¡tica de learning rate\")\n",
    "    print(\"      - DetecÃ§Ã£o de problemas de convergÃªncia\")\n",
    "    print(\"      - SugestÃµes de ajustes de hiperparÃ¢metros\")\n",
    "    print(\"      - RelatÃ³rios detalhados de progresso\")\n",
    "\n",
    "# Executar treinamento com output em tempo real\n",
    "print(f\"\\nğŸ“ Comando: {cmd}\")\n",
    "print(\"ğŸ Iniciando treinamento...\\n\")\n",
    "\n",
    "try:\n",
    "    # Executar treinamento\n",
    "    run_command(cmd, f\"Treinamento VITS2 {CONFIG_NAME.upper()}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ğŸ‰ TREINAMENTO CONCLUÃDO COM SUCESSO!\")\n",
    "\n",
    "    # Verificar checkpoints gerados\n",
    "    checkpoint_dir = config[\"logging\"][\"checkpoint\"][\"dirpath\"]\n",
    "    if os.path.exists(checkpoint_dir):\n",
    "        checkpoints = [f for f in os.listdir(checkpoint_dir) if f.endswith('.ckpt')]\n",
    "        print(f\"ğŸ“ Checkpoints gerados: {len(checkpoints)}\")\n",
    "\n",
    "        if checkpoints:\n",
    "            # Mostrar checkpoints mais recentes\n",
    "            checkpoint_paths = [os.path.join(checkpoint_dir, f) for f in checkpoints]\n",
    "            checkpoint_paths.sort(key=os.path.getmtime, reverse=True)\n",
    "\n",
    "            print(\"ğŸ“¦ Checkpoints disponÃ­veis:\")\n",
    "            for i, ckpt in enumerate(checkpoint_paths[:3]):  # Mostrar 3 mais recentes\n",
    "                size_mb = os.path.getsize(ckpt) / 1024 / 1024\n",
    "                print(f\"   {i+1}. {os.path.basename(ckpt)} ({size_mb:.1f} MB)\")\n",
    "\n",
    "    # Verificar logs do TensorBoard\n",
    "    tensorboard_dir = config[\"logging\"][\"tensorboard\"][\"save_dir\"]\n",
    "    if os.path.exists(tensorboard_dir):\n",
    "        print(f\"ğŸ“Š Logs TensorBoard: {tensorboard_dir}\")\n",
    "\n",
    "    # Verificar logs do Monitor LLM se ativo\n",
    "    if LLM_CONFIG['enabled']:\n",
    "        llm_logs_dir = \"logs/llm_monitor\"\n",
    "        if os.path.exists(llm_logs_dir):\n",
    "            llm_files = [f for f in os.listdir(llm_logs_dir) if f.endswith('.json')]\n",
    "            print(f\"ğŸ¤– AnÃ¡lises LLM geradas: {len(llm_files)}\")\n",
    "            if llm_files:\n",
    "                latest_analysis = sorted(llm_files)[-1]\n",
    "                print(f\"   ğŸ“„ Ãšltima anÃ¡lise: {latest_analysis}\")\n",
    "\n",
    "    print(f\"\\nâœ… Modelo treinado com sucesso para inglÃªs!\")\n",
    "    print(f\"ğŸ¯ ConfiguraÃ§Ã£o: {CONFIG_NAME}\")\n",
    "    print(f\"ğŸŒ Idioma: InglÃªs\")\n",
    "    print(f\"ğŸ“ˆ Dataset: 22.910 amostras\")\n",
    "    print(f\"ğŸ¤– Monitor LLM: {'Usado' if LLM_CONFIG['enabled'] else 'NÃ£o usado'}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ ERRO DURANTE TREINAMENTO:\")\n",
    "    print(f\"   {str(e)}\")\n",
    "    print(f\"\\nğŸ” DiagnÃ³stico:\")\n",
    "\n",
    "    # Verificar se arquivos necessÃ¡rios existem\n",
    "    required_files = [\n",
    "        config_path,\n",
    "        config[\"data\"][\"metadata_file\"],\n",
    "        config[\"data\"][\"data_dir\"]\n",
    "    ]\n",
    "\n",
    "    for file_path in required_files:\n",
    "        if os.path.exists(file_path):\n",
    "            print(f\"   âœ… {file_path}\")\n",
    "        else:\n",
    "            print(f\"   âŒ {file_path} - NÃƒO ENCONTRADO\")\n",
    "\n",
    "    # Verificar GPU\n",
    "    try:\n",
    "        import torch\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"   âœ… CUDA disponÃ­vel: {torch.cuda.get_device_name(0)}\")\n",
    "        else:\n",
    "            print(\"   âŒ CUDA nÃ£o disponÃ­vel\")\n",
    "    except:\n",
    "        print(\"   âŒ PyTorch nÃ£o instalado ou erro CUDA\")\n",
    "\n",
    "    # Verificar configuraÃ§Ã£o LLM se ativa\n",
    "    if LLM_CONFIG['enabled']:\n",
    "        if os.environ.get('OPENROUTER_API_KEY'):\n",
    "            print(\"   âœ… API Key OpenRouter configurada\")\n",
    "        else:\n",
    "            print(\"   âŒ API Key OpenRouter nÃ£o encontrada\")\n",
    "\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === VISUALIZAÃ‡ÃƒO DE AMOSTRAS GERADAS ===\n",
    "# Reproduzir e analisar amostras de Ã¡udio geradas durante o treinamento\n",
    "\n",
    "import glob\n",
    "import os\n",
    "from IPython.display import Audio, display, HTML\n",
    "\n",
    "# Verificar se existem amostras geradas\n",
    "samples_dir = \"samples\"\n",
    "if not os.path.exists(samples_dir):\n",
    "    print(\"ğŸ“ Nenhuma amostra encontrada ainda\")\n",
    "    print(\"ğŸ’¡ As amostras serÃ£o geradas automaticamente durante o treinamento\")\n",
    "else:\n",
    "    # Encontrar todas as amostras por Ã©poca\n",
    "    sample_files = glob.glob(f\"{samples_dir}/**/*.wav\", recursive=True)\n",
    "\n",
    "    if not sample_files:\n",
    "        print(\"ğŸµ Nenhuma amostra de Ã¡udio encontrada ainda\")\n",
    "        print(\"â³ Aguarde o treinamento gerar as primeiras amostras\")\n",
    "    else:\n",
    "        print(f\"ğŸµ Encontradas {len(sample_files)} amostras de Ã¡udio!\")\n",
    "\n",
    "        # Organizar por Ã©poca\n",
    "        samples_by_epoch = {}\n",
    "        for sample_file in sample_files:\n",
    "            # Extrair nÃºmero da Ã©poca do nome do arquivo\n",
    "            basename = os.path.basename(sample_file)\n",
    "            if \"epoch_\" in basename:\n",
    "                try:\n",
    "                    epoch_num = int(basename.split(\"epoch_\")[1].split(\"_\")[0])\n",
    "                    if epoch_num not in samples_by_epoch:\n",
    "                        samples_by_epoch[epoch_num] = []\n",
    "                    samples_by_epoch[epoch_num].append(sample_file)\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "        if samples_by_epoch:\n",
    "            print(f\"ğŸ“Š Amostras organizadas por {len(samples_by_epoch)} Ã©pocas\")\n",
    "\n",
    "            # Mostrar amostras das Ãºltimas 3 Ã©pocas\n",
    "            recent_epochs = sorted(samples_by_epoch.keys())[-3:]\n",
    "\n",
    "            for epoch in recent_epochs:\n",
    "                print(f\"\\nğŸ¯ Ã‰poca {epoch:03d}:\")\n",
    "                epoch_samples = samples_by_epoch[epoch]\n",
    "\n",
    "                for i, sample_file in enumerate(epoch_samples[:3]):  # MÃ¡ximo 3 por Ã©poca\n",
    "                    print(f\"   ğŸµ Amostra {i+1}:\")\n",
    "\n",
    "                    # Extrair informaÃ§Ãµes do nome do arquivo\n",
    "                    basename = os.path.basename(sample_file)\n",
    "                    if \"speaker_\" in basename:\n",
    "                        speaker_id = basename.split(\"speaker_\")[1].split(\"_\")[0]\n",
    "                        print(f\"      ğŸ‘¤ Speaker: {speaker_id}\")\n",
    "\n",
    "                    # Reproduzir Ã¡udio\n",
    "                    try:\n",
    "                        display(Audio(sample_file, rate=22050))\n",
    "\n",
    "                        # Mostrar tamanho do arquivo\n",
    "                        size_kb = os.path.getsize(sample_file) / 1024\n",
    "                        print(f\"      ğŸ“¦ Tamanho: {size_kb:.1f} KB\")\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f\"      âŒ Erro ao carregar: {e}\")\n",
    "\n",
    "            # EstatÃ­sticas gerais\n",
    "            print(f\"\\nğŸ“ˆ EstatÃ­sticas das Amostras:\")\n",
    "            print(f\"   ğŸµ Total de amostras: {len(sample_files)}\")\n",
    "            print(f\"   ğŸ“Š Ã‰pocas com amostras: {len(samples_by_epoch)}\")\n",
    "\n",
    "            # Tamanho total\n",
    "            total_size = sum(os.path.getsize(f) for f in sample_files)\n",
    "            print(f\"   ğŸ’¾ Tamanho total: {total_size / 1024 / 1024:.1f} MB\")\n",
    "\n",
    "            # Ãšltima Ã©poca\n",
    "            if recent_epochs:\n",
    "                last_epoch = max(recent_epochs)\n",
    "                print(f\"   ğŸ¯ Ãšltima Ã©poca: {last_epoch}\")\n",
    "\n",
    "        else:\n",
    "            print(\"âš ï¸ Amostras encontradas mas nÃ£o organizadas por Ã©poca\")\n",
    "            print(\"ğŸ’¡ Verifique o formato dos nomes dos arquivos\")\n",
    "\n",
    "# Widget para navegaÃ§Ã£o (se houver muitas amostras)\n",
    "if 'samples_by_epoch' in locals() and len(samples_by_epoch) > 3:\n",
    "    print(f\"\\nğŸ›ï¸ Use o cÃ³digo abaixo para navegar pelas amostras de Ã©pocas especÃ­ficas:\")\n",
    "    print(f\"```python\")\n",
    "    print(f\"# Escolha uma Ã©poca especÃ­fica\")\n",
    "    print(f\"epoch_to_play = 10  # Substitua pelo nÃºmero da Ã©poca\")\n",
    "    print(f\"if epoch_to_play in samples_by_epoch:\")\n",
    "    print(f\"    for sample in samples_by_epoch[epoch_to_play]:\")\n",
    "    print(f\"        display(Audio(sample, rate=22050))\")\n",
    "    print(f\"```\")\n",
    "\n",
    "print(\"\\nğŸ¯ Esta cÃ©lula serÃ¡ atualizada automaticamente conforme novas amostras sÃ£o geradas!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === DOWNLOAD DOS RESULTADOS ===\n",
    "import zipfile\n",
    "from datetime import datetime\n",
    "import glob\n",
    "\n",
    "# Verificar se as variÃ¡veis necessÃ¡rias estÃ£o definidas\n",
    "try:\n",
    "    CONFIG_NAME\n",
    "    config\n",
    "except NameError as e:\n",
    "    print(f\"âŒ ERRO: VariÃ¡vel nÃ£o definida - {e}\")\n",
    "    print(\"ğŸ’¡ Execute as cÃ©lulas anteriores primeiro!\")\n",
    "    print(\"   Especialmente a cÃ©lula de 'ConfiguraÃ§Ã£o do modelo'\")\n",
    "    raise SystemExit(\"Execute as cÃ©lulas anteriores primeiro!\")\n",
    "\n",
    "# Criar arquivo ZIP com resultados\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "zip_filename = f\"valetts_vits2_english_{CONFIG_NAME.split('_')[-1]}_{timestamp}.zip\"\n",
    "\n",
    "print(f\"ğŸ“¦ Criando arquivo ZIP: {zip_filename}\")\n",
    "print(\"ğŸ“‹ Incluindo todos os tipos de checkpoints...\")\n",
    "\n",
    "with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "\n",
    "    # Adicionar checkpoints principais (2 mais recentes)\n",
    "    checkpoint_dir = config[\"logging\"][\"checkpoint\"][\"dirpath\"]\n",
    "    recent_checkpoints = []\n",
    "    if os.path.exists(checkpoint_dir):\n",
    "        recent_ckpts = glob.glob(f\"{checkpoint_dir}/*.ckpt\")\n",
    "        if recent_ckpts:\n",
    "            recent_checkpoints = sorted(recent_ckpts, key=os.path.getmtime)[-2:]  # 2 mais recentes\n",
    "            for ckpt in recent_checkpoints:\n",
    "                zipf.write(ckpt, os.path.relpath(ckpt, '.'))\n",
    "            print(f\"   ğŸ“¦ Checkpoints recentes: {len(recent_checkpoints)} adicionados\")\n",
    "\n",
    "    # Adicionar checkpoints de backup (a cada 10 Ã©pocas)\n",
    "    backup_dir = config[\"logging\"][\"checkpoint_backup\"][\"dirpath\"]\n",
    "    backup_checkpoints = []\n",
    "    if os.path.exists(backup_dir):\n",
    "        backup_ckpts = glob.glob(f\"{backup_dir}/*.ckpt\")\n",
    "        if backup_ckpts:\n",
    "            backup_checkpoints = sorted(backup_ckpts, key=os.path.getmtime)\n",
    "            for ckpt in backup_checkpoints:\n",
    "                zipf.write(ckpt, os.path.relpath(ckpt, '.'))\n",
    "            print(f\"   ğŸ”’ Checkpoints backup: {len(backup_checkpoints)} adicionados\")\n",
    "\n",
    "    # Adicionar configuraÃ§Ã£o\n",
    "    try:\n",
    "        config_path\n",
    "        zipf.write(config_path, config_path)\n",
    "        print(f\"   âš™ï¸ ConfiguraÃ§Ã£o: {config_path} adicionada\")\n",
    "    except NameError:\n",
    "        print(\"   âš ï¸ config_path nÃ£o definido - configuraÃ§Ã£o nÃ£o incluÃ­da\")\n",
    "\n",
    "    # Adicionar amostras geradas\n",
    "    samples_added = 0\n",
    "    if os.path.exists(\"samples\"):\n",
    "        for root, dirs, files in os.walk(\"samples\"):\n",
    "            for file in files:\n",
    "                if file.endswith('.wav'):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    zipf.write(file_path, os.path.relpath(file_path, '.'))\n",
    "                    samples_added += 1\n",
    "        print(f\"   ğŸµ Amostras de Ã¡udio: {samples_added} adicionadas\")\n",
    "\n",
    "    # Adicionar logs principais\n",
    "    logs_added = 0\n",
    "    log_files = [\"logs/training.log\", \"logs/tensorboard\"]\n",
    "    for log_item in log_files:\n",
    "        if os.path.exists(log_item):\n",
    "            if os.path.isfile(log_item):\n",
    "                zipf.write(log_item, log_item)\n",
    "                logs_added += 1\n",
    "            elif os.path.isdir(log_item):\n",
    "                for root, dirs, files in os.walk(log_item):\n",
    "                    for file in files:\n",
    "                        if not file.startswith('.'):  # Ignorar arquivos ocultos\n",
    "                            file_path = os.path.join(root, file)\n",
    "                            zipf.write(file_path, os.path.relpath(file_path, '.'))\n",
    "                            logs_added += 1\n",
    "    if logs_added > 0:\n",
    "        print(f\"   ğŸ“Š Logs: {logs_added} arquivos adicionados\")\n",
    "\n",
    "print(f\"\\nâœ… Arquivo criado: {zip_filename}\")\n",
    "print(f\"ğŸ’¾ Tamanho: {os.path.getsize(zip_filename) / 1024 / 1024:.1f} MB\")\n",
    "\n",
    "# EstatÃ­sticas detalhadas dos checkpoints\n",
    "print(f\"\\nğŸ“Š Resumo dos Checkpoints:\")\n",
    "if recent_checkpoints:\n",
    "    print(f\"   ğŸ“¦ Checkpoints Recentes ({len(recent_checkpoints)}):\")\n",
    "    for i, ckpt in enumerate(recent_checkpoints):\n",
    "        size_mb = os.path.getsize(ckpt) / 1024 / 1024\n",
    "        epoch = \"???\"\n",
    "        try:\n",
    "            # Extrair Ã©poca do nome do arquivo\n",
    "            import re\n",
    "            match = re.search(r'epoch_(\\d+)', os.path.basename(ckpt))\n",
    "            if not match:\n",
    "                match = re.search(r'-(\\d+)-', os.path.basename(ckpt))\n",
    "            if match:\n",
    "                epoch = match.group(1)\n",
    "        except:\n",
    "            pass\n",
    "        print(f\"      {i+1}. Ã‰poca {epoch} - {size_mb:.1f}MB\")\n",
    "\n",
    "if backup_checkpoints:\n",
    "    print(f\"   ğŸ”’ Checkpoints Backup ({len(backup_checkpoints)}):\")\n",
    "    for i, ckpt in enumerate(backup_checkpoints):\n",
    "        size_mb = os.path.getsize(ckpt) / 1024 / 1024\n",
    "        epoch = \"???\"\n",
    "        try:\n",
    "            import re\n",
    "            match = re.search(r'epoch_(\\d+)', os.path.basename(ckpt))\n",
    "            if not match:\n",
    "                match = re.search(r'-(\\d+)-', os.path.basename(ckpt))\n",
    "            if match:\n",
    "                epoch = match.group(1)\n",
    "        except:\n",
    "            pass\n",
    "        print(f\"      {i+1}. Ã‰poca {epoch} - {size_mb:.1f}MB\")\n",
    "\n",
    "# Download no Colab\n",
    "try:\n",
    "    if 'google.colab' in sys.modules:\n",
    "        from google.colab import files\n",
    "        print(\"\\nâ¬‡ï¸ Iniciando download...\")\n",
    "        files.download(zip_filename)\n",
    "        print(\"âœ… Download concluÃ­do!\")\n",
    "except:\n",
    "    print(\"\\nâ„¹ï¸ NÃ£o estÃ¡ no Colab ou erro no download - arquivo salvo localmente\")\n",
    "\n",
    "print(\"\\nğŸ¯ TREINAMENTO FINALIZADO!\")\n",
    "try:\n",
    "    print(f\"ğŸ“Š Modo: {CONFIG_NAME.upper()}\")\n",
    "    try:\n",
    "        DEBUG_MODE\n",
    "        print(f\"â±ï¸ Status: {'Teste concluÃ­do' if DEBUG_MODE else 'ProduÃ§Ã£o concluÃ­da'}\")\n",
    "    except NameError:\n",
    "        print(\"â±ï¸ Status: ConcluÃ­do\")\n",
    "except NameError:\n",
    "    print(\"ğŸ“Š Modo: NÃ£o definido\")\n",
    "\n",
    "print(f\"ğŸ“¦ Resultados salvos em: {zip_filename}\")\n",
    "print(f\"\\nğŸ’¾ Sistema HÃ­brido de Checkpoints implementado:\")\n",
    "print(f\"   ğŸ“¦ Checkpoints recentes: {len(recent_checkpoints) if recent_checkpoints else 0} (Ãºltimos 2)\")\n",
    "print(f\"   ğŸ”’ Checkpoints backup: {len(backup_checkpoints) if backup_checkpoints else 0} (a cada 10 Ã©pocas)\")\n",
    "print(f\"   ğŸµ Amostras de Ã¡udio: {samples_added} arquivos\")\n",
    "\n",
    "# Verificar espaÃ§o total usado\n",
    "try:\n",
    "    total_recent = sum(os.path.getsize(f) for f in recent_checkpoints) / 1024 / 1024 if recent_checkpoints else 0\n",
    "    total_backup = sum(os.path.getsize(f) for f in backup_checkpoints) / 1024 / 1024 if backup_checkpoints else 0\n",
    "    print(f\"\\nğŸ’¾ EspaÃ§o em disco utilizado:\")\n",
    "    print(f\"   ğŸ“¦ Checkpoints recentes: {total_recent:.1f}MB\")\n",
    "    print(f\"   ğŸ”’ Checkpoints backup: {total_backup:.1f}MB\")\n",
    "    print(f\"   ğŸ“Š Total checkpoints: {total_recent + total_backup:.1f}MB\")\n",
    "except:\n",
    "    print(\"\\nâš ï¸ NÃ£o foi possÃ­vel calcular espaÃ§o em disco\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
