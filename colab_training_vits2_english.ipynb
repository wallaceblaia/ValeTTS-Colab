{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": "# üé§ ValeTTS VITS2 English Training - Vers√£o Otimizada\n## Treinamento Completo do Modelo VITS2 para Ingl√™s no Google Colab\n\n**Status**: ‚úÖ **FUNCIONAL** - Sistema completo otimizado para A100\n\n### üöÄ Recursos Implementados:\n- ‚úÖ Suporte completo ao ingl√™s com phonemes G2P\n- ‚úÖ Detec√ß√£o autom√°tica de GPU (A100/V100/T4)\n- ‚úÖ Configura√ß√µes otimizadas por GPU\n- ‚úÖ Monitor LLM OpenRouter integrado\n- ‚úÖ TensorBoard em tempo real\n- ‚úÖ Sistema de checkpoints otimizado (√∫ltimos 2)\n- ‚úÖ Gera√ß√£o de amostras a cada checkpoint\n- ‚úÖ Multi-speaker (52 falantes)\n- ‚úÖ Dataset: 22.910 amostras validadas\n\n### ‚ö° Performance por GPU:\n- **A100 40GB**: Batch 32 ‚Üí 5-8 it/s ‚Üí 4-6 horas\n- **V100 16GB**: Batch 24 ‚Üí 3-5 it/s ‚Üí 6-8 horas  \n- **T4 15GB**: Batch 16 ‚Üí 1-2 it/s ‚Üí 12-15 horas\n\n### üìã Ordem de Execu√ß√£o:\n1. **Configura√ß√£o da API OpenRouter** (opcional)\n2. **Configura√ß√£o do Modo de Treinamento** \n3. **Setup do Sistema e Clonagem**\n4. **Instala√ß√£o de Depend√™ncias**\n5. **Download e Verifica√ß√£o do Dataset**\n6. **Configura√ß√£o Din√¢mica do Modelo**\n7. **Inicializa√ß√£o do TensorBoard**\n8. **Execu√ß√£o do Treinamento**\n9. **Download dos Resultados**",
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# === CONFIGURA√á√ÉO DA API OPENROUTER (OPCIONAL) ===\n# Monitor LLM para otimiza√ß√£o inteligente do treinamento\n\nimport os\nimport getpass\n\n# Configura√ß√£o da API OpenRouter\nENABLE_LLM_MONITOR = True # @param {type:\"boolean\"}\n\nif ENABLE_LLM_MONITOR:\n    print(\"ü§ñ Configurando Monitor LLM com OpenRouter...\")\n    print(\"üìã O monitor LLM analisa o progresso do treinamento e sugere ajustes autom√°ticos\")\n    print(\"üí° Opcional: deixe em branco para desabilitar\")\n    \n    # Input da API key (campo seguro)\n    OPENROUTER_API_KEY = getpass.getpass(\"üîë Digite sua API Key do OpenRouter (ou Enter para pular): \")\n    \n    if OPENROUTER_API_KEY and OPENROUTER_API_KEY.strip():\n        # Configurar vari√°vel de ambiente\n        os.environ['OPENROUTER_API_KEY'] = OPENROUTER_API_KEY.strip()\n        \n        # Testar conex√£o\n        try:\n            import requests\n            test_url = \"https://openrouter.ai/api/v1/models\"\n            headers = {\n                \"Authorization\": f\"Bearer {OPENROUTER_API_KEY.strip()}\",\n                \"Content-Type\": \"application/json\"\n            }\n            \n            response = requests.get(test_url, headers=headers, timeout=10)\n            if response.status_code == 200:\n                print(\"‚úÖ API Key v√°lida - Monitor LLM habilitado\")\n                print(\"üìä Funcionalidades ativas:\")\n                print(\"   - An√°lise autom√°tica a cada 10 √©pocas\")\n                print(\"   - Sugest√µes de ajuste de learning rate\")\n                print(\"   - Detec√ß√£o de problemas de converg√™ncia\")\n                print(\"   - Relat√≥rios detalhados de progresso\")\n                \n                # Mostrar modelos dispon√≠veis\n                models = response.json().get('data', [])\n                claude_models = [m for m in models if 'claude' in m.get('id', '').lower()]\n                if claude_models:\n                    print(f\"üß† Modelo recomendado: {claude_models[0].get('id', 'claude-3-5-sonnet')}\")\n            else:\n                print(f\"‚ö†Ô∏è Erro na valida√ß√£o da API: {response.status_code}\")\n                print(\"üîß Monitor LLM ser√° desabilitado para este treinamento\")\n                ENABLE_LLM_MONITOR = False\n                \n        except Exception as e:\n            print(f\"‚ö†Ô∏è Erro ao testar API: {e}\")\n            print(\"üîß Monitor LLM ser√° desabilitado para este treinamento\")\n            ENABLE_LLM_MONITOR = False\n    else:\n        print(\"‚ÑπÔ∏è API Key n√£o fornecida - Monitor LLM desabilitado\")\n        ENABLE_LLM_MONITOR = False\nelse:\n    print(\"üîß Monitor LLM desabilitado por configura√ß√£o\")\n    ENABLE_LLM_MONITOR = False\n\n# Salvar configura√ß√£o para uso posterior\nLLM_CONFIG = {\n    'enabled': ENABLE_LLM_MONITOR,\n    'provider': 'openrouter',\n    'model': 'anthropic/claude-3-5-sonnet-20241022',\n    'monitor_every_epochs': 10,\n    'api_key_configured': bool(os.environ.get('OPENROUTER_API_KEY'))\n}\n\nprint(f\"\\nüéØ Configura√ß√£o LLM: {'‚úÖ Ativo' if ENABLE_LLM_MONITOR else '‚ùå Desabilitado'}\")\n\nif not ENABLE_LLM_MONITOR:\n    print(\"\\nüí° Para habilitar o Monitor LLM:\")\n    print(\"1. Crie uma conta em https://openrouter.ai\")\n    print(\"2. Obtenha sua API Key\")\n    print(\"3. Execute esta c√©lula novamente e forne√ßa a key\")\n    print(\"4. O monitor ajudar√° a otimizar seu treinamento automaticamente\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# === CONFIGURA√á√ÉO DO MODO DE TREINAMENTO ===\n# Configura√ß√£o otimizada para A100 40GB VRAM\nDEBUG_MODE = False # @param {type:\"boolean\"}\nUSE_DRIVE = True # @param {type:\"boolean\"}\nMOUNT_DRIVE = True # @param {type:\"boolean\"}\n\n# Detectar GPU automaticamente e ajustar configura√ß√£o\nimport subprocess\ndef get_gpu_info():\n    try:\n        result = subprocess.run(['nvidia-smi', '--query-gpu=name,memory.total', '--format=csv,noheader,nounits'], \n                              capture_output=True, text=True)\n        if result.returncode == 0:\n            gpu_info = result.stdout.strip().split('\\n')[0]\n            gpu_name, gpu_memory = gpu_info.split(', ')\n            return gpu_name.strip(), int(gpu_memory.strip())\n    except:\n        pass\n    return \"Unknown\", 0\n\ngpu_name, gpu_memory = get_gpu_info()\nprint(f\"üéÆ GPU Detectada: {gpu_name}\")\nprint(f\"üíæ VRAM: {gpu_memory} MB\")\n\n# Configura√ß√£o autom√°tica baseada na GPU\nif \"A100\" in gpu_name and gpu_memory >= 40000:\n    print(\"üöÄ MODO A100: Configura√ß√£o de alta performance\")\n    BATCH_SIZE = 32\n    CONFIG_NAME = \"vits2_english_a100_optimized\"\n    EXPECTED_PERFORMANCE = \"5-8 it/s\"\n    ESTIMATED_TIME = \"4-6 horas\"\nelif \"V100\" in gpu_name:\n    print(\"‚ö° MODO V100: Configura√ß√£o otimizada\")\n    BATCH_SIZE = 24\n    CONFIG_NAME = \"vits2_english_production\"\n    EXPECTED_PERFORMANCE = \"3-5 it/s\"\n    ESTIMATED_TIME = \"6-8 horas\"\nelif \"T4\" in gpu_name:\n    print(\"üì± MODO T4: Configura√ß√£o conservativa\")\n    BATCH_SIZE = 16\n    CONFIG_NAME = \"vits2_english_production\"\n    EXPECTED_PERFORMANCE = \"1-2 it/s\"\n    ESTIMATED_TIME = \"12-15 horas\"\nelse:\n    print(\"‚ö†Ô∏è GPU n√£o detectada, usando configura√ß√£o padr√£o\")\n    BATCH_SIZE = 16\n    CONFIG_NAME = \"vits2_english_production\"\n    EXPECTED_PERFORMANCE = \"1-2 it/s\"\n    ESTIMATED_TIME = \"12-15 horas\"\n\n# Configura√ß√£o do treinamento\nif DEBUG_MODE:\n    print(\"üêõ MODO DEBUG: Treinamento r√°pido para teste\")\n    EPOCHS = 3\n    MAX_SAMPLES = 100\n    CONFIG_NAME = \"vits2_english_debug\"\n    BATCH_SIZE = min(BATCH_SIZE, 8)  # Reduzido para debug\nelse:\n    print(\"üöÄ MODO PRODU√á√ÉO: Treinamento completo\")\n    EPOCHS = 200\n    MAX_SAMPLES = None\n\nprint(f\"üìä Configura√ß√£o: {CONFIG_NAME}\")\nprint(f\"üîÑ √âpocas: {EPOCHS}\")\nprint(f\"üì¶ Batch Size: {BATCH_SIZE}\")\nprint(f\"üìà Amostras: {MAX_SAMPLES if MAX_SAMPLES else 'Todas (22.910)'}\")\nprint(f\"‚ö° Performance esperada: {EXPECTED_PERFORMANCE}\")\nprint(f\"‚è±Ô∏è Tempo estimado: {ESTIMATED_TIME}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CONFIGURA√á√ÉO DO SISTEMA E CLONAGEM ===\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "\n",
    "def run_command(cmd, description):\n",
    "    \"\"\"Executa comando com output em tempo real.\"\"\"\n",
    "    print(f\"üîÑ {description}\")\n",
    "    process = subprocess.Popen(\n",
    "        cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT,\n",
    "        universal_newlines=True, bufsize=1\n",
    "    )\n",
    "\n",
    "    for line in process.stdout:\n",
    "        print(line.rstrip())\n",
    "\n",
    "    process.wait()\n",
    "    if process.returncode != 0:\n",
    "        raise RuntimeError(f\"Comando falhou: {cmd}\")\n",
    "    print(f\"‚úÖ {description} - Conclu√≠do\\n\")\n",
    "\n",
    "# Montar Google Drive se necess√°rio\n",
    "if MOUNT_DRIVE:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    if USE_DRIVE:\n",
    "        drive_path = '/content/drive/MyDrive/ValeTTS'\n",
    "        os.makedirs(drive_path, exist_ok=True)\n",
    "        os.chdir(drive_path)\n",
    "        print(f\"üìÅ Diret√≥rio de trabalho: {drive_path}\")\n",
    "\n",
    "# Verificar GPU\n",
    "run_command(\"nvidia-smi\", \"Verificando GPU dispon√≠vel\")\n",
    "\n",
    "# Clonar reposit√≥rio\n",
    "if not os.path.exists('ValeTTS'):\n",
    "    run_command(\n",
    "        \"git clone https://github.com/wallaceblaia/ValeTTS-Colab.git ValeTTS\",\n",
    "        \"Clonando reposit√≥rio ValeTTS\"\n",
    "    )\n",
    "else:\n",
    "    print(\"üìÅ Reposit√≥rio j√° existe\")\n",
    "\n",
    "os.chdir('ValeTTS')\n",
    "run_command(\"git pull origin main\", \"Atualizando reposit√≥rio\")\n",
    "print(f\"üìç Diret√≥rio atual: {os.getcwd()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# === INSTALA√á√ÉO DE DEPEND√äNCIAS ESPEC√çFICAS PARA INGL√äS ===\n\nprint(\"üîß Instalando depend√™ncias do sistema para ingl√™s...\")\n\n# Instalar depend√™ncias do sistema para ingl√™s\nsystem_deps = [\n    \"apt-get update\",\n    \"apt-get install -y espeak espeak-data libespeak1 libespeak-dev\",\n    \"apt-get install -y ffmpeg sox libsox-fmt-all\",\n    \"apt-get install -y language-pack-en\",  # Suporte espec√≠fico para ingl√™s\n]\n\nfor cmd in system_deps:\n    run_command(cmd, f\"Sistema: {cmd.split()[-1]}\")\n\nprint(\"üì¶ Instalando depend√™ncias Python...\")\n\n# Lista de depend√™ncias Python essenciais\nessential_deps = [\n    \"pip install phonemizer==3.3.0\",  # G2P essencial\n    \"pip install num2words\",          # Convers√£o n√∫meros para ingl√™s\n    \"pip install inflect\",            # Pluraliza√ß√£o e ordinais em ingl√™s\n]\n\n# Lista de depend√™ncias opcionais\noptional_deps = [\n    (\"pip install eSpeak\", \"eSpeak Python interface\"),\n    (\"pip install espeak-ng\", \"eSpeak-NG alternativa\"),\n]\n\n# Instalar depend√™ncias essenciais\nfor cmd in essential_deps:\n    run_command(cmd, f\"Python essencial: {cmd.split()[-1]}\")\n\n# Tentar instalar depend√™ncias opcionais\nfor cmd, desc in optional_deps:\n    try:\n        run_command(cmd, f\"Python opcional: {desc}\")\n        break  # Se uma funcionar, n√£o precisamos das outras\n    except RuntimeError:\n        print(f\"‚ö†Ô∏è {desc} n√£o dispon√≠vel, tentando pr√≥xima...\")\n\nprint(\"üîß Instalando ValeTTS...\")\n# Instalar ValeTTS em modo desenvolvimento\nrun_command(\"pip install -e .\", \"Instalando ValeTTS em modo desenvolvimento\")\n\n# Verificar instala√ß√µes cr√≠ticas\nprint(\"üß™ Verificando instala√ß√µes...\")\n\n# Verifica√ß√µes essenciais\nessential_checks = [\n    (\"python -c 'import phonemizer; print(f\\\"‚úÖ Phonemizer: {phonemizer.__version__}\\\")'\", \"Phonemizer\"),\n    (\"python -c 'import valetts; print(f\\\"‚úÖ ValeTTS instalado\\\")'\", \"ValeTTS\"),\n]\n\n# Verifica√ß√µes opcionais\noptional_checks = [\n    (\"espeak --version\", \"ESpeak sistema\"),\n    (\"python -c 'from valetts.data.preprocessing.text_en import EnglishTextPreprocessor; print(\\\"‚úÖ EnglishTextPreprocessor dispon√≠vel\\\")'\", \"Processador ingl√™s\"),\n]\n\n# Executar verifica√ß√µes essenciais\nfor cmd, desc in essential_checks:\n    run_command(cmd, f\"Verificar {desc}\")\n\n# Executar verifica√ß√µes opcionais\nfor cmd, desc in optional_checks:\n    try:\n        run_command(cmd, f\"Verificar {desc}\")\n    except Exception as e:\n        print(f\"‚ö†Ô∏è Aviso: {desc} - {e}\")\n\n# Teste robusto do processador de ingl√™s\nprint(\"üß™ Testando processamento de ingl√™s...\")\n\ntest_code_robust = '''\nimport sys\nimport os\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nprint(\"üîç Testando importa√ß√µes...\")\n\n# Teste 1: Importar ValeTTS\ntry:\n    import valetts\n    print(\"‚úÖ ValeTTS importado\")\nexcept Exception as e:\n    print(f\"‚ùå ValeTTS: {e}\")\n    sys.exit(1)\n\n# Teste 2: Importar processador de texto ingl√™s\ntry:\n    from valetts.data.preprocessing.text_en import EnglishTextPreprocessor\n    print(\"‚úÖ EnglishTextPreprocessor importado\")\nexcept Exception as e:\n    print(f\"‚ùå EnglishTextPreprocessor: {e}\")\n    sys.exit(1)\n\n# Teste 3: Criar processador (sem phonemes primeiro)\ntry:\n    processor_basic = EnglishTextPreprocessor(\n        language=\"en-us\",\n        use_phonemes=False,  # Desabilitar phonemes para teste b√°sico\n        normalize_numbers=True\n    )\n    print(\"‚úÖ Processador b√°sico criado\")\nexcept Exception as e:\n    print(f\"‚ùå Processador b√°sico: {e}\")\n    sys.exit(1)\n\n# Teste 4: Normaliza√ß√£o b√°sica\ntry:\n    test_text = \"Hello world! This is a test with numbers 123 and Mr. Smith.\"\n    normalized = processor_basic.normalize_text(test_text)\n    print(f\"‚úÖ Texto normalizado: {normalized}\")\nexcept Exception as e:\n    print(f\"‚ùå Normaliza√ß√£o: {e}\")\n\n# Teste 5: Processador com phonemes (opcional)\ntry:\n    processor_phonemes = EnglishTextPreprocessor(\n        language=\"en-us\",\n        use_phonemes=True,\n        normalize_numbers=True\n    )\n    print(\"‚úÖ Processador com phonemes criado\")\n    \n    # Teste phonemes\n    try:\n        phonemes = processor_phonemes.text_to_phonemes(\"hello world\")\n        print(f\"‚úÖ Phonemes: {phonemes}\")\n    except Exception as e:\n        print(f\"‚ö†Ô∏è Phonemes: {e} - Funcionalidade opcional\")\n        \nexcept Exception as e:\n    print(f\"‚ö†Ô∏è Processador com phonemes: {e} - Funcionalidade opcional\")\n\nprint(\"\\\\n‚úÖ Teste b√°sico do processador conclu√≠do!\")\nprint(\"üí° O sistema est√° pronto para treinamento mesmo se phonemes falharam\")\n'''\n\n# Executar teste robusto\ntry:\n    with open(\"test_english_robust.py\", \"w\") as f:\n        f.write(test_code_robust)\n    run_command(\"python test_english_robust.py\", \"Teste robusto do processador ingl√™s\")\nexcept Exception as e:\n    print(f\"‚ùå Teste falhou: {e}\")\n    print(\"üí° Pode haver problemas com as depend√™ncias\")\nfinally:\n    # Limpar arquivo de teste\n    if os.path.exists(\"test_english_robust.py\"):\n        os.remove(\"test_english_robust.py\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"‚úÖ INSTALA√á√ÉO CONCLU√çDA!\")\nprint(\"=\"*60)\nprint(\"üéØ Sistema configurado para treinamento VITS2 em ingl√™s\")\nprint(\"\")\nprint(\"üìã Componentes instalados:\")\nprint(\"   ‚úÖ ESpeak (sistema) - para G2P em ingl√™s\")\nprint(\"   ‚úÖ Phonemizer 3.3.0 - convers√£o texto‚Üíphonema\")\nprint(\"   ‚úÖ num2words - n√∫meros em ingl√™s\")\nprint(\"   ‚úÖ inflect - pluraliza√ß√£o em ingl√™s\")\nprint(\"   ‚úÖ ValeTTS - framework principal\")\nprint(\"   ‚úÖ EnglishTextPreprocessor - processamento espec√≠fico\")\nprint(\"\")\nprint(\"üí° NOTAS IMPORTANTES:\")\nprint(\"   - Se phonemes falharam, o treinamento ainda funciona\")\nprint(\"   - O sistema usar√° fallback para processamento de texto\")\nprint(\"   - A performance pode ser ligeiramente reduzida mas funcional\")\nprint(\"\")\nprint(\"üöÄ PR√ìXIMO PASSO: Execute a c√©lula de download do dataset!\")\n\n# Verifica√ß√£o final dos imports cr√≠ticos\nprint(\"\\nüîç Verifica√ß√£o final...\")\ntry:\n    import valetts\n    from valetts.data.preprocessing.text_en import EnglishTextPreprocessor\n    from valetts.models.vits2.model import VITS2\n    from valetts.models.vits2.config import VITS2Config\n    print(\"‚úÖ Todos os imports cr√≠ticos funcionando!\")\nexcept Exception as e:\n    print(f\"‚ùå Problema cr√≠tico: {e}\")\n    print(\"üîÑ Pode ser necess√°rio reiniciar o runtime e tentar novamente\")"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# === DOWNLOAD E VERIFICA√á√ÉO COMPLETA DO DATASET ===\nimport os\nimport pandas as pd\nfrom pathlib import Path\n\nprint(\"üì• INICIANDO VERIFICA√á√ÉO COMPLETA DO DATASET...\")\nprint(\"=\"*60)\n\n# Configura√ß√£o de caminhos\ndataset_path = \"data/generated/Dataset-Unificado\"\nmetadata_file = f\"{dataset_path}/metadata.csv\"\naudio_dir = f\"{dataset_path}/audio\"\naudio_raw_dir = f\"{dataset_path}/audio/raw\"\n\nprint(f\"üìÅ Caminhos configurados:\")\nprint(f\"   Dataset: {dataset_path}\")\nprint(f\"   Metadata: {metadata_file}\")\nprint(f\"   √Åudio: {audio_raw_dir}\")\n\n# ETAPA 1: Verificar se dataset existe localmente\nprint(f\"\\nüîç ETAPA 1: Verificando exist√™ncia local...\")\nif os.path.exists(metadata_file):\n    print(\"‚úÖ Dataset encontrado localmente\")\n    print(f\"   üìÑ Metadata: {metadata_file}\")\n    print(f\"   üìä Tamanho: {os.path.getsize(metadata_file):,} bytes\")\nelse:\n    print(\"üì• Dataset n√£o encontrado localmente\")\n    \n    # ETAPA 2: Verificar Google Drive\n    print(f\"\\nüîç ETAPA 2: Verificando Google Drive...\")\n    drive_dataset = \"/content/drive/MyDrive/ValeTTS/data/generated/Dataset-Unificado\"\n    \n    if os.path.exists(drive_dataset):\n        print(f\"‚úÖ Dataset encontrado no Drive!\")\n        print(f\"   üìÅ Local: {drive_dataset}\")\n        \n        # Mostrar conte√∫do do Drive\n        drive_contents = os.listdir(drive_dataset)\n        print(f\"   üìã Conte√∫do: {drive_contents}\")\n        \n        print(f\"\\nüîó Criando estrutura local...\")\n        os.makedirs(\"data/generated\", exist_ok=True)\n        \n        # Verificar se j√° existe link/diret√≥rio\n        if os.path.exists(dataset_path):\n            if os.path.islink(dataset_path):\n                print(\"   üóëÔ∏è Removendo link simb√≥lico anterior...\")\n                os.unlink(dataset_path)\n            elif os.path.isdir(dataset_path):\n                print(\"   ‚ö†Ô∏è Diret√≥rio j√° existe, usando existente\")\n        \n        if not os.path.exists(dataset_path):\n            print(\"   üîó Criando link simb√≥lico...\")\n            os.symlink(drive_dataset, dataset_path)\n            print(\"   ‚úÖ Link criado com sucesso!\")\n    else:\n        print(\"‚ùå Dataset n√£o encontrado no Drive\")\n        print(f\"   üìÅ Procurado em: {drive_dataset}\")\n        \n        print(f\"\\nüí° SOLU√á√ïES PARA OBTER O DATASET:\")\n        print(f\"\")\n        print(f\"üîß M√âTODO 1 - Google Drive (RECOMENDADO):\")\n        print(f\"   1. Fa√ßa upload do dataset para:\")\n        print(f\"      {drive_dataset}\")\n        print(f\"   2. Execute esta c√©lula novamente\")\n        print(f\"\")\n        print(f\"üîß M√âTODO 2 - Upload direto:\")\n        print(f\"   1. Use o √≠cone üìÅ no painel esquerdo\")\n        print(f\"   2. Carregue para: {dataset_path}\")\n        print(f\"\")\n        print(f\"üîß M√âTODO 3 - Download autom√°tico:\")\n        print(f\"   Substitua SUA_URL pela URL do Google Drive:\")\n        print(f\"   !gdown --folder 'SUA_URL' -O {dataset_path}\")\n        \n        # Parar aqui se n√£o encontrou o dataset\n        print(f\"\\n‚õî PARANDO: Dataset n√£o encontrado!\")\n        print(f\"üìã Execute uma das solu√ß√µes acima primeiro.\")\n\n# ETAPA 3: Verifica√ß√£o detalhada do dataset\nif os.path.exists(metadata_file):\n    print(f\"\\nüîç ETAPA 3: Analisando estrutura do dataset...\")\n    \n    # Verificar estrutura de diret√≥rios\n    print(f\"üìÅ Verificando estrutura:\")\n    structure_checks = [\n        (dataset_path, \"Dataset base\"),\n        (audio_dir, \"Pasta audio\"),\n        (audio_raw_dir, \"Pasta audio/raw\"),\n        (metadata_file, \"Arquivo metadata.csv\")\n    ]\n    \n    for path, description in structure_checks:\n        exists = os.path.exists(path)\n        if exists:\n            if os.path.isfile(path):\n                size = os.path.getsize(path)\n                print(f\"   ‚úÖ {description}: {size:,} bytes\")\n            else:\n                items = len(os.listdir(path))\n                print(f\"   ‚úÖ {description}: {items:,} itens\")\n        else:\n            print(f\"   ‚ùå {description}: N√ÉO ENCONTRADO\")\n    \n    # ETAPA 4: An√°lise do metadata.csv\n    print(f\"\\nüîç ETAPA 4: Carregando e analisando metadata...\")\n    try:\n        print(f\"üìä Carregando CSV...\")\n        df = pd.read_csv(metadata_file)\n        \n        total_samples = len(df)\n        unique_speakers = df['speaker_id'].nunique()\n        locales = df['locale'].unique() if 'locale' in df.columns else ['unknown']\n        \n        print(f\"‚úÖ Metadata carregado com sucesso!\")\n        print(f\"   üìà Total de amostras: {total_samples:,}\")\n        print(f\"   üé§ Falantes √∫nicos: {unique_speakers}\")\n        print(f\"   üåç Idiomas: {locales}\")\n        print(f\"   üìã Colunas: {list(df.columns)}\")\n        \n        # Verificar idioma ingl√™s\n        if 'locale' in df.columns:\n            english_samples = len(df[df['locale'] == 'en']) if 'en' in locales else 0\n            print(f\"   üá∫üá∏ Amostras em ingl√™s: {english_samples:,}\")\n            \n            if english_samples == 0:\n                print(\"   ‚ùå PROBLEMA: Nenhuma amostra em ingl√™s!\")\n            elif english_samples < 1000:\n                print(\"   ‚ö†Ô∏è AVISO: Poucas amostras para treinamento robusto\")\n            else:\n                print(\"   ‚úÖ Quantidade adequada para treinamento\")\n        \n        # Verificar colunas obrigat√≥rias\n        required_columns = ['audio_path', 'text_normalized', 'speaker_id']\n        missing_columns = [col for col in required_columns if col not in df.columns]\n        \n        if missing_columns:\n            print(f\"   ‚ùå PROBLEMA: Colunas ausentes: {missing_columns}\")\n        else:\n            print(f\"   ‚úÖ Todas as colunas obrigat√≥rias presentes\")\n        \n        # ETAPA 5: Verifica√ß√£o dos arquivos de √°udio\n        print(f\"\\nüîç ETAPA 5: Verificando arquivos de √°udio...\")\n        \n        # Contar arquivos reais\n        if os.path.exists(audio_raw_dir):\n            wav_files = [f for f in os.listdir(audio_raw_dir) if f.endswith('.wav')]\n            total_wav_files = len(wav_files)\n            print(f\"üìä Arquivos WAV encontrados: {total_wav_files:,}\")\n            \n            # Verificar correspond√™ncia\n            csv_entries = len(df)\n            if total_wav_files == csv_entries:\n                print(f\"‚úÖ PERFEITO: N√∫mero de arquivos corresponde ao CSV!\")\n            elif total_wav_files == csv_entries - 1:  # Considerando header\n                print(f\"‚úÖ BOM: Arquivos correspondem (considerando header do CSV)\")\n            else:\n                print(f\"‚ö†Ô∏è DIVERG√äNCIA: CSV={csv_entries}, WAV={total_wav_files}\")\n            \n            # Verificar primeiros arquivos\n            print(f\"\\nüîç Testando primeiros 10 arquivos...\")\n            sample_size = min(10, len(df))\n            found_count = 0\n            \n            for i in range(sample_size):\n                audio_path_csv = df.iloc[i]['audio_path']\n                full_path = os.path.join(dataset_path, audio_path_csv)\n                \n                if os.path.exists(full_path):\n                    size = os.path.getsize(full_path)\n                    found_count += 1\n                    print(f\"   ‚úÖ {audio_path_csv} ({size:,} bytes)\")\n                else:\n                    print(f\"   ‚ùå {audio_path_csv} - N√ÉO ENCONTRADO\")\n            \n            print(f\"\\nüìä Resultado da amostra: {found_count}/{sample_size} arquivos encontrados\")\n            \n            # Verificar tamanhos dos arquivos\n            if found_count > 0:\n                print(f\"\\nüìä Estat√≠sticas dos arquivos:\")\n                sizes = []\n                for i in range(min(100, len(df))):  # Amostra de 100 arquivos\n                    audio_path_csv = df.iloc[i]['audio_path']\n                    full_path = os.path.join(dataset_path, audio_path_csv)\n                    if os.path.exists(full_path):\n                        sizes.append(os.path.getsize(full_path))\n                \n                if sizes:\n                    avg_size = sum(sizes) / len(sizes)\n                    min_size = min(sizes)\n                    max_size = max(sizes)\n                    print(f\"   üìè Tamanho m√©dio: {avg_size:,.0f} bytes\")\n                    print(f\"   üìè Menor arquivo: {min_size:,} bytes\")\n                    print(f\"   üìè Maior arquivo: {max_size:,} bytes\")\n            \n            # Verificar formato dos arquivos\n            print(f\"\\nüîç Verificando formato do primeiro arquivo...\")\n            if found_count > 0:\n                first_audio = os.path.join(dataset_path, df.iloc[0]['audio_path'])\n                import subprocess\n                try:\n                    result = subprocess.run(['file', first_audio], capture_output=True, text=True)\n                    print(f\"   üéµ Formato: {result.stdout.strip()}\")\n                except:\n                    print(f\"   ‚ö†Ô∏è N√£o foi poss√≠vel verificar o formato\")\n        \n        else:\n            print(f\"‚ùå Pasta de √°udio n√£o encontrada: {audio_raw_dir}\")\n        \n        # ETAPA 6: Resumo final\n        print(f\"\\n\" + \"=\"*60)\n        print(f\"üìä RESUMO FINAL DO DATASET:\")\n        print(f\"=\"*60)\n        print(f\"‚úÖ Metadata: {total_samples:,} amostras\")\n        print(f\"‚úÖ Falantes: {unique_speakers}\")\n        print(f\"‚úÖ Idioma: ingl√™s ({english_samples:,} amostras)\")\n        print(f\"‚úÖ Arquivos WAV: {total_wav_files:,}\")\n        print(f\"‚úÖ Correspond√™ncia: {found_count}/{sample_size} testados\")\n        \n        # Status final\n        all_good = (\n            english_samples > 0 and \n            not missing_columns and \n            total_wav_files > 0 and \n            found_count == sample_size\n        )\n        \n        if all_good:\n            print(f\"\\nüöÄ DATASET COMPLETAMENTE PRONTO!\")\n            print(f\"‚úÖ Pode prosseguir para o treinamento\")\n            print(f\"üìä Performance esperada: excelente com {english_samples:,} amostras\")\n        else:\n            print(f\"\\n‚ö†Ô∏è DATASET TEM PROBLEMAS MENORES\")\n            print(f\"üîß Verifique os pontos marcados acima\")\n            print(f\"üí° O treinamento pode ainda assim funcionar\")\n        \n        # Mostrar amostra dos dados\n        print(f\"\\nüìã Amostra dos dados:\")\n        sample_cols = ['speaker_id', 'text_normalized', 'locale']\n        available_cols = [col for col in sample_cols if col in df.columns]\n        print(df.head(3)[available_cols].to_string())\n        \n    except Exception as e:\n        print(f\"‚ùå ERRO ao analisar metadata: {e}\")\n        import traceback\n        traceback.print_exc()\n\nelse:\n    print(f\"\\n‚ùå DATASET N√ÉO DISPON√çVEL\")\n    print(f\"‚ö†Ô∏è Execute os m√©todos de download mostrados acima\")\n\nprint(f\"\\nüìç STATUS FINAL:\")\nprint(f\"   Dataset: {'‚úÖ' if os.path.exists(dataset_path) else '‚ùå'}\")\nprint(f\"   Metadata: {'‚úÖ' if os.path.exists(metadata_file) else '‚ùå'}\")\nprint(f\"   √Åudio: {'‚úÖ' if os.path.exists(audio_raw_dir) else '‚ùå'}\")\nprint(f\"   Pronto: {'‚úÖ SIM' if os.path.exists(metadata_file) else '‚ùå N√ÉO'}\")\n\nif os.path.exists(metadata_file):\n    print(f\"\\nüéØ PR√ìXIMO PASSO: Execute a c√©lula de configura√ß√£o do modelo!\")\nelse:\n    print(f\"\\nüéØ PR√ìXIMO PASSO: Configure o dataset primeiro!\")"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# === CONFIGURA√á√ÉO DIN√ÇMICA DO MODELO PARA INGL√äS ===\nimport yaml\nimport pandas as pd\n\n# Primeiro, verificar o dataset\nmetadata_file = \"data/generated/Dataset-Unificado/metadata.csv\"\nif os.path.exists(metadata_file):\n    df = pd.read_csv(metadata_file)\n    total_samples = len(df)\n    unique_speakers = df['speaker_id'].nunique()\n    locales = df['locale'].unique()\n    \n    print(f\"üìä Dataset carregado:\")\n    print(f\"   üìà Total: {total_samples:,} amostras\")\n    print(f\"   üé§ Falantes: {unique_speakers}\")\n    print(f\"   üåç Idiomas: {locales}\")\n    \n    # Verificar se √© ingl√™s\n    if 'en' not in locales:\n        print(\"‚ö†Ô∏è AVISO: Dataset n√£o cont√©m ingl√™s!\")\n        print(f\"   Idiomas encontrados: {locales}\")\n    else:\n        english_samples = len(df[df['locale'] == 'en'])\n        print(f\"   ‚úÖ Amostras em ingl√™s: {english_samples:,}\")\nelse:\n    print(\"‚ùå Metadata n√£o encontrada!\")\n    total_samples = 22910  # Fallback\n    unique_speakers = 52\n\n# Usar configura√ß√£o espec√≠fica para A100 se dispon√≠vel\nif CONFIG_NAME == \"vits2_english_a100_optimized\":\n    # Carregar configura√ß√£o A100 otimizada\n    config_path = f\"configs/training/{CONFIG_NAME}.yaml\"\n    if os.path.exists(config_path):\n        with open(config_path, 'r') as f:\n            config = yaml.safe_load(f)\n        print(\"‚úÖ Usando configura√ß√£o A100 otimizada!\")\n    else:\n        print(\"‚ö†Ô∏è Configura√ß√£o A100 n√£o encontrada, criando...\")\n        config = {}\nelse:\n    config = {}\n\n# Configura√ß√£o base ou atualiza√ß√£o\nif not config or 'model' not in config:\n    if DEBUG_MODE:\n        model_config = {\n            \"text_encoder_hidden_dim\": 128,\n            \"latent_dim\": 128,\n            \"speaker_embedding_dim\": 256,\n            \"generator_initial_channels\": 256,\n            \"decoder_hidden_dim\": 256,\n        }\n    else:\n        # Configura√ß√£o baseada na GPU\n        if \"A100\" in gpu_name:\n            model_config = {\n                \"text_encoder_hidden_dim\": 512,\n                \"latent_dim\": 512,\n                \"speaker_embedding_dim\": 1024,\n                \"generator_initial_channels\": 1024,\n                \"decoder_hidden_dim\": 1024,\n            }\n        else:\n            model_config = {\n                \"text_encoder_hidden_dim\": 192,\n                \"latent_dim\": 192,\n                \"speaker_embedding_dim\": 512,\n                \"generator_initial_channels\": 512,\n                \"decoder_hidden_dim\": 512,\n            }\n    \n    config = {\n        \"model\": {\n            \"name\": \"VITS2\",\n            \"mel_channels\": 80,\n            \"n_speakers\": unique_speakers,\n            \"text_processor\": \"english\",\n            \"inference_only\": False,\n            **model_config\n        }\n    }\n\n# Atualizar configura√ß√µes de treinamento\nconfig.update({\n    \"training\": {\n        \"learning_rate\": 2.0e-4,\n        \"batch_size\": BATCH_SIZE,\n        \"max_epochs\": EPOCHS,\n        \"accumulate_grad_batches\": 1,\n        \"max_grad_norm\": 1.0,\n        \"mel_loss_weight\": 45.0,\n        \"kl_loss_weight\": 1.0,\n        \"adv_loss_weight\": 1.0,\n        \"fm_loss_weight\": 2.0,\n        \"duration_loss_weight\": 1.0,\n        \"use_amp\": True,\n        \"gradient_clip_val\": 1.0,\n        \"discriminator_update_frequency\": 1,\n        \"scheduler\": {\n            \"name\": \"ReduceLROnPlateau\",\n            \"mode\": \"min\",\n            \"factor\": 0.5,\n            \"patience\": 15 if not DEBUG_MODE else 5,\n            \"min_lr\": 1.0e-6\n        }\n    },\n    \"data\": {\n        \"dataset_format\": \"valetts\",\n        \"data_dir\": \"data/generated/Dataset-Unificado\",\n        \"metadata_file\": \"data/generated/Dataset-Unificado/metadata.csv\",\n        \"language\": \"en\",  # CR√çTICO: usar \"en\" para ingl√™s\n        \"locale_column\": \"locale\",\n        \"text_processor\": {\n            \"class\": \"EnglishTextPreprocessor\",\n            \"use_phonemes\": True,\n            \"normalize_numbers\": True,\n            \"normalize_whitespace\": True,\n            \"lowercase\": True,\n            \"backend\": \"espeak\",\n            \"language\": \"en-us\"\n        },\n        \"sample_rate\": 22050,\n        \"n_mels\": 80,\n        \"n_fft\": 1024,\n        \"hop_length\": 256,\n        \"win_length\": 1024,\n        \"num_workers\": 8 if \"A100\" in gpu_name else 4,\n        \"pin_memory\": True,\n        \"persistent_workers\": True,\n        \"use_augmentation\": True,\n        \"volume_range\": [0.9, 1.1],\n        \"pitch_range\": [-1, 1]\n    },\n    \"logging\": {\n        \"log_dir\": \"logs\",\n        \"experiment_name\": f\"vits2_english_{CONFIG_NAME.split('_')[-1]}\",\n        \n        # SISTEMA H√çBRIDO DE CHECKPOINTS\n        \"checkpoint\": {\n            # Checkpoints principais (2 √∫ltimos - se sobrescrevem)\n            \"dirpath\": f\"checkpoints/vits2_english_{CONFIG_NAME.split('_')[-1]}\",\n            \"filename\": f\"vits2_english-{{epoch:03d}}-{{epoch/val_loss_total:.3f}}\",\n            \"monitor\": \"epoch/val_loss_total\",\n            \"mode\": \"min\",\n            \"save_top_k\": 2,  # Apenas 2 checkpoints recentes\n            \"save_last\": True,\n            \"every_n_epochs\": 1,  # Salvar a cada √©poca\n            \"auto_insert_metric_name\": False,\n            \"save_on_train_epoch_end\": True\n        },\n        \n        # Checkpoints de backup (a cada 10 √©pocas - permanentes)\n        \"checkpoint_backup\": {\n            \"enabled\": True,\n            \"dirpath\": f\"checkpoints/vits2_english_{CONFIG_NAME.split('_')[-1]}/backup\",\n            \"filename\": f\"vits2_english_backup-{{epoch:03d}}-{{epoch/val_loss_total:.3f}}\",\n            \"every_n_epochs\": 10,  # Backup a cada 10 √©pocas\n            \"save_top_k\": -1,  # Salvar todos os backups (nunca sobrescrever)\n            \"monitor\": \"epoch/val_loss_total\",\n            \"mode\": \"min\"\n        },\n        \n        \"early_stopping\": {\n            \"monitor\": \"epoch/val_loss_total\",\n            \"mode\": \"min\",\n            \"patience\": 10 if DEBUG_MODE else 30,\n            \"min_delta\": 0.001\n        },\n        \n        \"tensorboard\": {\n            \"save_dir\": \"logs/tensorboard\",\n            \"name\": f\"vits2_english_{CONFIG_NAME.split('_')[-1]}\"\n        }\n    },\n    \"hardware\": {\n        \"accelerator\": \"gpu\",\n        \"devices\": 1,\n        \"precision\": \"16-mixed\",\n        \"strategy\": \"auto\",\n        \"benchmark\": True if \"A100\" in gpu_name else False\n    },\n    \"validation\": {\n        \"val_check_interval\": 1.0,\n        \"generate_samples\": True,\n        \"sample_every_n_epochs\": 1,  # Gerar amostras a cada √©poca\n        \"limit_val_batches\": 1.0,\n        \"num_sanity_val_steps\": 2\n    },\n    \"dataset_config\": {\n        \"expected_locale\": \"en\",  # CR√çTICO: valor correto para ingl√™s\n        \"validate_files\": True,\n        \"cache_preprocessing\": True,\n        \"audio_column\": \"audio_path\",\n        \"text_column\": \"text_normalized\",\n        \"speaker_column\": \"speaker_id\",\n        \"locale_column\": \"locale\"\n    },\n    \n    # CONFIGURA√á√ÉO LLM INTEGRADA\n    \"llm_monitor\": LLM_CONFIG,\n    \n    # CONFIGURA√á√ÉO DE AMOSTRAS DE √ÅUDIO\n    \"audio_sampling\": {\n        \"enabled\": True,\n        \"sample_every_n_epochs\": 1,  # Gerar amostras a cada √©poca\n        \"num_samples\": 3,  # 3 amostras por checkpoint\n        \"max_length\": 10.0,  # M√°ximo 10 segundos\n        \"sample_rate\": 22050,\n        \"save_dir\": \"samples\",\n        \"speakers_to_sample\": [0, 1, 2]  # Primeiros 3 speakers\n    }\n})\n\n# Adicionar limita√ß√£o de amostras para debug\nif DEBUG_MODE:\n    config[\"data\"][\"max_samples_debug\"] = MAX_SAMPLES\n\n# Criar diret√≥rios necess√°rios\nos.makedirs(\"configs/training\", exist_ok=True)\nos.makedirs(config[\"logging\"][\"checkpoint\"][\"dirpath\"], exist_ok=True)\nos.makedirs(config[\"logging\"][\"checkpoint_backup\"][\"dirpath\"], exist_ok=True)  # Diret√≥rio backup\nos.makedirs(\"logs/tensorboard\", exist_ok=True)\nos.makedirs(\"samples\", exist_ok=True)\n\n# Salvar configura√ß√£o\nconfig_path = f\"configs/training/{CONFIG_NAME}.yaml\"\nwith open(config_path, 'w') as f:\n    yaml.dump(config, f, default_flow_style=False, indent=2)\n\nprint(f\"‚úÖ Configura√ß√£o criada: {config_path}\")\nprint(f\"üéØ Modo: {CONFIG_NAME.upper()}\")\nprint(f\"üìä √âpocas: {EPOCHS}\")\nprint(f\"üì¶ Batch Size: {BATCH_SIZE}\")\nprint(f\"üé§ Falantes: {config['model']['n_speakers']}\")\nprint(f\"üåç Idioma: INGL√äS (locale='en')\")\nprint(f\"ü§ñ Monitor LLM: {'‚úÖ Ativo' if LLM_CONFIG['enabled'] else '‚ùå Desabilitado'}\")\n\nprint(f\"\\nüíæ Sistema H√≠brido de Checkpoints:\")\nprint(f\"   üì¶ Checkpoints recentes: 2 (se sobrescrevem a cada √©poca)\")\nprint(f\"   üîí Checkpoints backup: A cada 10 √©pocas (permanentes)\")\nprint(f\"   üìÅ Diret√≥rio principal: {config['logging']['checkpoint']['dirpath']}\")\nprint(f\"   üóÑÔ∏è Diret√≥rio backup: {config['logging']['checkpoint_backup']['dirpath']}\")\nprint(f\"   üéµ Amostras de √°udio: A cada √©poca\")\n\n# Estimar espa√ßo em disco\nepochs_to_run = EPOCHS\nrecent_checkpoints_size = 2 * 150  # 2 checkpoints √ó ~150MB cada\nbackup_checkpoints_count = (epochs_to_run // 10) + 1\nbackup_checkpoints_size = backup_checkpoints_count * 150\ntotal_checkpoints_size = recent_checkpoints_size + backup_checkpoints_size\n\nprint(f\"\\nüíæ Estimativa de Espa√ßo em Disco:\")\nprint(f\"   üì¶ Checkpoints recentes: ~{recent_checkpoints_size}MB\")\nprint(f\"   üîí Checkpoints backup: ~{backup_checkpoints_size}MB ({backup_checkpoints_count} arquivos)\")\nprint(f\"   üéµ Amostras de √°udio: ~{epochs_to_run * 3 * 0.03:.1f}MB\")\nprint(f\"   üìä Total estimado: ~{total_checkpoints_size + (epochs_to_run * 3 * 0.03):.0f}MB\")\n\nprint(f\"\\nüíæ Dimens√µes do modelo:\")\nprint(f\"   - Hidden: {config['model']['text_encoder_hidden_dim']}\")\nprint(f\"   - Speaker: {config['model']['speaker_embedding_dim']}\")\nprint(f\"   - Generator: {config['model']['generator_initial_channels']}\")\nprint(f\"   - Decoder: {config['model']['decoder_hidden_dim']}\")\n\n# Verificar compatibilidade de dimens√µes CR√çTICA\nif config['model']['speaker_embedding_dim'] == config['model']['decoder_hidden_dim']:\n    print(\"‚úÖ Dimens√µes compat√≠veis - Sem erro de tensor!\")\nelse:\n    print(\"‚ùå AVISO: Dimens√µes incompat√≠veis detectadas!\")\n    print(f\"   Speaker: {config['model']['speaker_embedding_dim']}\")\n    print(f\"   Decoder: {config['model']['decoder_hidden_dim']}\")\n\nprint(f\"\\nüöÄ Configura√ß√£o otimizada para {gpu_name} pronta!\")\n\nif LLM_CONFIG['enabled']:\n    print(\"\\nü§ñ Monitor LLM ativo - benef√≠cios:\")\n    print(\"   üìä An√°lise inteligente do progresso\")\n    print(\"   ‚öôÔ∏è Ajustes autom√°ticos de hiperpar√¢metros\")\n    print(\"   üéØ Detec√ß√£o precoce de problemas\")\n    print(\"   üìà Relat√≥rios detalhados de treinamento\")"
  },
  {
   "cell_type": "code",
   "source": "# === INICIALIZA√á√ÉO DO TENSORBOARD ===\n# Configurar TensorBoard antes do treinamento para monitoramento em tempo real\n\n# Verificar se as vari√°veis necess√°rias est√£o definidas\ntry:\n    config\n    CONFIG_NAME\nexcept NameError as e:\n    print(f\"‚ùå ERRO: Execute a c√©lula de configura√ß√£o do modelo primeiro!\")\n    raise SystemExit(\"Execute as c√©lulas anteriores primeiro!\")\n\nprint(\"üìä Configurando TensorBoard para monitoramento em tempo real...\")\n\n# Configurar diret√≥rio do TensorBoard\ntensorboard_dir = config[\"logging\"][\"tensorboard\"][\"save_dir\"]\nexperiment_name = config[\"logging\"][\"tensorboard\"][\"name\"]\n\nprint(f\"üìÅ Diret√≥rio: {tensorboard_dir}\")\nprint(f\"üéØ Experimento: {experiment_name}\")\n\n# Criar diret√≥rio se n√£o existir\nos.makedirs(tensorboard_dir, exist_ok=True)\n\n# Inicializar TensorBoard no Colab\ntry:\n    # Carregar extens√£o do TensorBoard\n    %load_ext tensorboard\n    \n    # Iniciar TensorBoard\n    %tensorboard --logdir={tensorboard_dir} --reload_interval=30\n    \n    print(\"‚úÖ TensorBoard iniciado com sucesso!\")\n    print(\"üìä Acesse a aba 'TensorBoard' acima para ver os gr√°ficos em tempo real\")\n    print(f\"üîÑ Atualiza√ß√£o autom√°tica a cada 30 segundos\")\n    print(\"\\nüìà M√©tricas dispon√≠veis:\")\n    print(\"   - Loss total de treinamento e valida√ß√£o\")\n    print(\"   - Loss MEL, KL, Adversarial, Feature Matching\")\n    print(\"   - Learning rate e gradient norm\")\n    print(\"   - Amostras de √°udio geradas\")\n    print(\"   - Gr√°ficos de converg√™ncia\")\n    \n    if LLM_CONFIG['enabled']:\n        print(\"   - An√°lises do Monitor LLM (a cada 10 √©pocas)\")\n    \nexcept Exception as e:\n    print(f\"‚ö†Ô∏è Erro ao inicializar TensorBoard: {e}\")\n    print(\"üí° TensorBoard ser√° inicializado automaticamente durante o treinamento\")\n\nprint(\"\\nüéØ Sistema pronto para treinamento com monitoramento completo!\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# === INICIAR TREINAMENTO OTIMIZADO PARA INGL√äS ===\n\n# Verificar se as vari√°veis necess√°rias est√£o definidas\ntry:\n    CONFIG_NAME\n    config_path\n    gpu_name\n    gpu_memory\n    BATCH_SIZE\n    ESTIMATED_TIME\n    EXPECTED_PERFORMANCE\n    LLM_CONFIG\nexcept NameError as e:\n    print(f\"‚ùå ERRO: Vari√°vel n√£o definida - {e}\")\n    print(\"üí° SOLU√á√ÉO: Execute as c√©lulas anteriores em ordem:\")\n    print(\"   1. Configura√ß√£o da API OpenRouter\")\n    print(\"   2. Configura√ß√£o do modo de treinamento\")\n    print(\"   3. Setup do sistema\")\n    print(\"   4. Instala√ß√£o de depend√™ncias\")\n    print(\"   5. Download do dataset\")\n    print(\"   6. Configura√ß√£o do modelo\")\n    print(\"   7. Esta c√©lula de treinamento\")\n    print(\"\")\n    print(\"üîÑ Execute novamente a partir da c√©lula 2 (Configura√ß√£o do modo)\")\n    raise SystemExit(\"Execute as c√©lulas anteriores primeiro!\")\n\nprint(f\"üöÄ Iniciando treinamento VITS2 - Modo: {CONFIG_NAME.upper()}\")\nprint(f\"üìÅ Configura√ß√£o: {config_path}\")\nprint(f\"üéÆ GPU: {gpu_name} ({gpu_memory} MB)\")\nprint(f\"üì¶ Batch Size: {BATCH_SIZE}\")\nprint(f\"‚è±Ô∏è Estimativa: {ESTIMATED_TIME}\")\nprint(f\"‚ö° Performance esperada: {EXPECTED_PERFORMANCE}\")\nprint(f\"ü§ñ Monitor LLM: {'‚úÖ Ativo' if LLM_CONFIG['enabled'] else '‚ùå Desabilitado'}\")\nprint(\"\\n\" + \"=\"*60)\n\n# Verificar se temos script espec√≠fico para ingl√™s\nenglish_script = \"scripts/train_vits2_english.py\"\nif os.path.exists(english_script):\n    print(\"‚úÖ Usando script espec√≠fico para ingl√™s\")\n    base_cmd = f\"python {english_script} --config {config_path}\"\nelse:\n    print(\"‚ö†Ô∏è Script espec√≠fico n√£o encontrado, usando script padr√£o\")\n    base_cmd = f\"python scripts/train_vits2.py --config {config_path}\"\n\n# Configurar comando baseado no LLM\nif LLM_CONFIG['enabled']:\n    print(\"ü§ñ Monitor LLM ativo - treinamento inteligente habilitado\")\n    cmd = base_cmd  # LLM j√° est√° na configura√ß√£o\nelse:\n    print(\"üîß Monitor LLM desabilitado - usando modo padr√£o\")\n    cmd = f\"{base_cmd} --disable-llm\"\n\n# Configurar CUDA para otimiza√ß√£o m√°xima\nos.environ['CUDA_LAUNCH_BLOCKING'] = '0'  # Async CUDA\nos.environ['TORCH_CUDNN_V8_API_ENABLED'] = '1'  # CuDNN v8\nos.environ['TORCH_ALLOW_TF32_CUBLAS_OVERRIDE'] = '1'  # TF32 para A100\n\nif \"A100\" in gpu_name:\n    print(\"üöÄ Configura√ß√µes especiais para A100:\")\n    print(\"   - TF32 habilitado para m√°xima performance\")\n    print(\"   - CuDNN v8 API ativado\")\n    print(\"   - Tensor cores ativados automaticamente\")\n    os.environ['TORCH_ALLOW_TF32'] = '1'\n    os.environ['TORCH_CUDNN_ALLOW_TF32'] = '1'\n\n# Configura√ß√µes espec√≠ficas para Monitor LLM\nif LLM_CONFIG['enabled']:\n    print(\"\\nü§ñ Configura√ß√µes do Monitor LLM:\")\n    print(f\"   üß† Modelo: {LLM_CONFIG['model']}\")\n    print(f\"   üìä An√°lise a cada: {LLM_CONFIG['monitor_every_epochs']} √©pocas\")\n    print(f\"   üîÑ Provider: {LLM_CONFIG['provider']}\")\n    print(\"   üìà Funcionalidades ativas:\")\n    print(\"      - Otimiza√ß√£o autom√°tica de learning rate\")\n    print(\"      - Detec√ß√£o de problemas de converg√™ncia\")\n    print(\"      - Sugest√µes de ajustes de hiperpar√¢metros\")\n    print(\"      - Relat√≥rios detalhados de progresso\")\n\n# Executar treinamento com output em tempo real\nprint(f\"\\nüìù Comando: {cmd}\")\nprint(\"üèÅ Iniciando treinamento...\\n\")\n\ntry:\n    # Executar treinamento\n    run_command(cmd, f\"Treinamento VITS2 {CONFIG_NAME.upper()}\")\n    \n    print(\"\\n\" + \"=\"*60)\n    print(\"üéâ TREINAMENTO CONCLU√çDO COM SUCESSO!\")\n    \n    # Verificar checkpoints gerados\n    checkpoint_dir = config[\"logging\"][\"checkpoint\"][\"dirpath\"]\n    if os.path.exists(checkpoint_dir):\n        checkpoints = [f for f in os.listdir(checkpoint_dir) if f.endswith('.ckpt')]\n        print(f\"üìÅ Checkpoints gerados: {len(checkpoints)}\")\n        \n        if checkpoints:\n            # Mostrar checkpoints mais recentes\n            checkpoint_paths = [os.path.join(checkpoint_dir, f) for f in checkpoints]\n            checkpoint_paths.sort(key=os.path.getmtime, reverse=True)\n            \n            print(\"üì¶ Checkpoints dispon√≠veis:\")\n            for i, ckpt in enumerate(checkpoint_paths[:3]):  # Mostrar 3 mais recentes\n                size_mb = os.path.getsize(ckpt) / 1024 / 1024\n                print(f\"   {i+1}. {os.path.basename(ckpt)} ({size_mb:.1f} MB)\")\n    \n    # Verificar logs do TensorBoard\n    tensorboard_dir = config[\"logging\"][\"tensorboard\"][\"save_dir\"]\n    if os.path.exists(tensorboard_dir):\n        print(f\"üìä Logs TensorBoard: {tensorboard_dir}\")\n    \n    # Verificar logs do Monitor LLM se ativo\n    if LLM_CONFIG['enabled']:\n        llm_logs_dir = \"logs/llm_monitor\"\n        if os.path.exists(llm_logs_dir):\n            llm_files = [f for f in os.listdir(llm_logs_dir) if f.endswith('.json')]\n            print(f\"ü§ñ An√°lises LLM geradas: {len(llm_files)}\")\n            if llm_files:\n                latest_analysis = sorted(llm_files)[-1]\n                print(f\"   üìÑ √öltima an√°lise: {latest_analysis}\")\n    \n    print(f\"\\n‚úÖ Modelo treinado com sucesso para ingl√™s!\")\n    print(f\"üéØ Configura√ß√£o: {CONFIG_NAME}\")\n    print(f\"üåç Idioma: Ingl√™s\")\n    print(f\"üìà Dataset: 22.910 amostras\")\n    print(f\"ü§ñ Monitor LLM: {'Usado' if LLM_CONFIG['enabled'] else 'N√£o usado'}\")\n\nexcept Exception as e:\n    print(f\"\\n‚ùå ERRO DURANTE TREINAMENTO:\")\n    print(f\"   {str(e)}\")\n    print(f\"\\nüîç Diagn√≥stico:\")\n    \n    # Verificar se arquivos necess√°rios existem\n    required_files = [\n        config_path,\n        config[\"data\"][\"metadata_file\"],\n        config[\"data\"][\"data_dir\"]\n    ]\n    \n    for file_path in required_files:\n        if os.path.exists(file_path):\n            print(f\"   ‚úÖ {file_path}\")\n        else:\n            print(f\"   ‚ùå {file_path} - N√ÉO ENCONTRADO\")\n    \n    # Verificar GPU\n    try:\n        import torch\n        if torch.cuda.is_available():\n            print(f\"   ‚úÖ CUDA dispon√≠vel: {torch.cuda.get_device_name(0)}\")\n        else:\n            print(\"   ‚ùå CUDA n√£o dispon√≠vel\")\n    except:\n        print(\"   ‚ùå PyTorch n√£o instalado ou erro CUDA\")\n    \n    # Verificar configura√ß√£o LLM se ativa\n    if LLM_CONFIG['enabled']:\n        if os.environ.get('OPENROUTER_API_KEY'):\n            print(\"   ‚úÖ API Key OpenRouter configurada\")\n        else:\n            print(\"   ‚ùå API Key OpenRouter n√£o encontrada\")\n    \n    raise"
  },
  {
   "cell_type": "code",
   "source": "# === VISUALIZA√á√ÉO DE AMOSTRAS GERADAS ===\n# Reproduzir e analisar amostras de √°udio geradas durante o treinamento\n\nimport glob\nimport os\nfrom IPython.display import Audio, display, HTML\n\n# Verificar se existem amostras geradas\nsamples_dir = \"samples\"\nif not os.path.exists(samples_dir):\n    print(\"üìÅ Nenhuma amostra encontrada ainda\")\n    print(\"üí° As amostras ser√£o geradas automaticamente durante o treinamento\")\nelse:\n    # Encontrar todas as amostras por √©poca\n    sample_files = glob.glob(f\"{samples_dir}/**/*.wav\", recursive=True)\n    \n    if not sample_files:\n        print(\"üéµ Nenhuma amostra de √°udio encontrada ainda\")\n        print(\"‚è≥ Aguarde o treinamento gerar as primeiras amostras\")\n    else:\n        print(f\"üéµ Encontradas {len(sample_files)} amostras de √°udio!\")\n        \n        # Organizar por √©poca\n        samples_by_epoch = {}\n        for sample_file in sample_files:\n            # Extrair n√∫mero da √©poca do nome do arquivo\n            basename = os.path.basename(sample_file)\n            if \"epoch_\" in basename:\n                try:\n                    epoch_num = int(basename.split(\"epoch_\")[1].split(\"_\")[0])\n                    if epoch_num not in samples_by_epoch:\n                        samples_by_epoch[epoch_num] = []\n                    samples_by_epoch[epoch_num].append(sample_file)\n                except:\n                    pass\n        \n        if samples_by_epoch:\n            print(f\"üìä Amostras organizadas por {len(samples_by_epoch)} √©pocas\")\n            \n            # Mostrar amostras das √∫ltimas 3 √©pocas\n            recent_epochs = sorted(samples_by_epoch.keys())[-3:]\n            \n            for epoch in recent_epochs:\n                print(f\"\\nüéØ √âpoca {epoch:03d}:\")\n                epoch_samples = samples_by_epoch[epoch]\n                \n                for i, sample_file in enumerate(epoch_samples[:3]):  # M√°ximo 3 por √©poca\n                    print(f\"   üéµ Amostra {i+1}:\")\n                    \n                    # Extrair informa√ß√µes do nome do arquivo\n                    basename = os.path.basename(sample_file)\n                    if \"speaker_\" in basename:\n                        speaker_id = basename.split(\"speaker_\")[1].split(\"_\")[0]\n                        print(f\"      üë§ Speaker: {speaker_id}\")\n                    \n                    # Reproduzir √°udio\n                    try:\n                        display(Audio(sample_file, rate=22050))\n                        \n                        # Mostrar tamanho do arquivo\n                        size_kb = os.path.getsize(sample_file) / 1024\n                        print(f\"      üì¶ Tamanho: {size_kb:.1f} KB\")\n                        \n                    except Exception as e:\n                        print(f\"      ‚ùå Erro ao carregar: {e}\")\n            \n            # Estat√≠sticas gerais\n            print(f\"\\nüìà Estat√≠sticas das Amostras:\")\n            print(f\"   üéµ Total de amostras: {len(sample_files)}\")\n            print(f\"   üìä √âpocas com amostras: {len(samples_by_epoch)}\")\n            \n            # Tamanho total\n            total_size = sum(os.path.getsize(f) for f in sample_files)\n            print(f\"   üíæ Tamanho total: {total_size / 1024 / 1024:.1f} MB\")\n            \n            # √öltima √©poca\n            if recent_epochs:\n                last_epoch = max(recent_epochs)\n                print(f\"   üéØ √öltima √©poca: {last_epoch}\")\n        \n        else:\n            print(\"‚ö†Ô∏è Amostras encontradas mas n√£o organizadas por √©poca\")\n            print(\"üí° Verifique o formato dos nomes dos arquivos\")\n\n# Widget para navega√ß√£o (se houver muitas amostras)\nif 'samples_by_epoch' in locals() and len(samples_by_epoch) > 3:\n    print(f\"\\nüéõÔ∏è Use o c√≥digo abaixo para navegar pelas amostras de √©pocas espec√≠ficas:\")\n    print(f\"```python\")\n    print(f\"# Escolha uma √©poca espec√≠fica\")\n    print(f\"epoch_to_play = 10  # Substitua pelo n√∫mero da √©poca\")\n    print(f\"if epoch_to_play in samples_by_epoch:\")\n    print(f\"    for sample in samples_by_epoch[epoch_to_play]:\")\n    print(f\"        display(Audio(sample, rate=22050))\")\n    print(f\"```\")\n\nprint(\"\\nüéØ Esta c√©lula ser√° atualizada automaticamente conforme novas amostras s√£o geradas!\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# === DOWNLOAD DOS RESULTADOS ===\nimport zipfile\nfrom datetime import datetime\nimport glob\n\n# Verificar se as vari√°veis necess√°rias est√£o definidas\ntry:\n    CONFIG_NAME\n    config\nexcept NameError as e:\n    print(f\"‚ùå ERRO: Vari√°vel n√£o definida - {e}\")\n    print(\"üí° Execute as c√©lulas anteriores primeiro!\")\n    print(\"   Especialmente a c√©lula de 'Configura√ß√£o do modelo'\")\n    raise SystemExit(\"Execute as c√©lulas anteriores primeiro!\")\n\n# Criar arquivo ZIP com resultados\ntimestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\nzip_filename = f\"valetts_vits2_english_{CONFIG_NAME.split('_')[-1]}_{timestamp}.zip\"\n\nprint(f\"üì¶ Criando arquivo ZIP: {zip_filename}\")\nprint(\"üìã Incluindo todos os tipos de checkpoints...\")\n\nwith zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:\n    \n    # Adicionar checkpoints principais (2 mais recentes)\n    checkpoint_dir = config[\"logging\"][\"checkpoint\"][\"dirpath\"]\n    recent_checkpoints = []\n    if os.path.exists(checkpoint_dir):\n        recent_ckpts = glob.glob(f\"{checkpoint_dir}/*.ckpt\")\n        if recent_ckpts:\n            recent_checkpoints = sorted(recent_ckpts, key=os.path.getmtime)[-2:]  # 2 mais recentes\n            for ckpt in recent_checkpoints:\n                zipf.write(ckpt, os.path.relpath(ckpt, '.'))\n            print(f\"   üì¶ Checkpoints recentes: {len(recent_checkpoints)} adicionados\")\n    \n    # Adicionar checkpoints de backup (a cada 10 √©pocas)\n    backup_dir = config[\"logging\"][\"checkpoint_backup\"][\"dirpath\"]\n    backup_checkpoints = []\n    if os.path.exists(backup_dir):\n        backup_ckpts = glob.glob(f\"{backup_dir}/*.ckpt\")\n        if backup_ckpts:\n            backup_checkpoints = sorted(backup_ckpts, key=os.path.getmtime)\n            for ckpt in backup_checkpoints:\n                zipf.write(ckpt, os.path.relpath(ckpt, '.'))\n            print(f\"   üîí Checkpoints backup: {len(backup_checkpoints)} adicionados\")\n\n    # Adicionar configura√ß√£o\n    try:\n        config_path\n        zipf.write(config_path, config_path)\n        print(f\"   ‚öôÔ∏è Configura√ß√£o: {config_path} adicionada\")\n    except NameError:\n        print(\"   ‚ö†Ô∏è config_path n√£o definido - configura√ß√£o n√£o inclu√≠da\")\n\n    # Adicionar amostras geradas\n    samples_added = 0\n    if os.path.exists(\"samples\"):\n        for root, dirs, files in os.walk(\"samples\"):\n            for file in files:\n                if file.endswith('.wav'):\n                    file_path = os.path.join(root, file)\n                    zipf.write(file_path, os.path.relpath(file_path, '.'))\n                    samples_added += 1\n        print(f\"   üéµ Amostras de √°udio: {samples_added} adicionadas\")\n\n    # Adicionar logs principais\n    logs_added = 0\n    log_files = [\"logs/training.log\", \"logs/tensorboard\"]\n    for log_item in log_files:\n        if os.path.exists(log_item):\n            if os.path.isfile(log_item):\n                zipf.write(log_item, log_item)\n                logs_added += 1\n            elif os.path.isdir(log_item):\n                for root, dirs, files in os.walk(log_item):\n                    for file in files:\n                        if not file.startswith('.'):  # Ignorar arquivos ocultos\n                            file_path = os.path.join(root, file)\n                            zipf.write(file_path, os.path.relpath(file_path, '.'))\n                            logs_added += 1\n    if logs_added > 0:\n        print(f\"   üìä Logs: {logs_added} arquivos adicionados\")\n\nprint(f\"\\n‚úÖ Arquivo criado: {zip_filename}\")\nprint(f\"üíæ Tamanho: {os.path.getsize(zip_filename) / 1024 / 1024:.1f} MB\")\n\n# Estat√≠sticas detalhadas dos checkpoints\nprint(f\"\\nüìä Resumo dos Checkpoints:\")\nif recent_checkpoints:\n    print(f\"   üì¶ Checkpoints Recentes ({len(recent_checkpoints)}):\")\n    for i, ckpt in enumerate(recent_checkpoints):\n        size_mb = os.path.getsize(ckpt) / 1024 / 1024\n        epoch = \"???\"\n        try:\n            # Extrair √©poca do nome do arquivo\n            import re\n            match = re.search(r'epoch_(\\d+)', os.path.basename(ckpt))\n            if not match:\n                match = re.search(r'-(\\d+)-', os.path.basename(ckpt))\n            if match:\n                epoch = match.group(1)\n        except:\n            pass\n        print(f\"      {i+1}. √âpoca {epoch} - {size_mb:.1f}MB\")\n\nif backup_checkpoints:\n    print(f\"   üîí Checkpoints Backup ({len(backup_checkpoints)}):\")\n    for i, ckpt in enumerate(backup_checkpoints):\n        size_mb = os.path.getsize(ckpt) / 1024 / 1024\n        epoch = \"???\"\n        try:\n            import re\n            match = re.search(r'epoch_(\\d+)', os.path.basename(ckpt))\n            if not match:\n                match = re.search(r'-(\\d+)-', os.path.basename(ckpt))\n            if match:\n                epoch = match.group(1)\n        except:\n            pass\n        print(f\"      {i+1}. √âpoca {epoch} - {size_mb:.1f}MB\")\n\n# Download no Colab\ntry:\n    if 'google.colab' in sys.modules:\n        from google.colab import files\n        print(\"\\n‚¨áÔ∏è Iniciando download...\")\n        files.download(zip_filename)\n        print(\"‚úÖ Download conclu√≠do!\")\nexcept:\n    print(\"\\n‚ÑπÔ∏è N√£o est√° no Colab ou erro no download - arquivo salvo localmente\")\n\nprint(\"\\nüéØ TREINAMENTO FINALIZADO!\")\ntry:\n    print(f\"üìä Modo: {CONFIG_NAME.upper()}\")\n    try:\n        DEBUG_MODE\n        print(f\"‚è±Ô∏è Status: {'Teste conclu√≠do' if DEBUG_MODE else 'Produ√ß√£o conclu√≠da'}\")\n    except NameError:\n        print(\"‚è±Ô∏è Status: Conclu√≠do\")\nexcept NameError:\n    print(\"üìä Modo: N√£o definido\")\n\nprint(f\"üì¶ Resultados salvos em: {zip_filename}\")\nprint(f\"\\nüíæ Sistema H√≠brido de Checkpoints implementado:\")\nprint(f\"   üì¶ Checkpoints recentes: {len(recent_checkpoints) if recent_checkpoints else 0} (√∫ltimos 2)\")\nprint(f\"   üîí Checkpoints backup: {len(backup_checkpoints) if backup_checkpoints else 0} (a cada 10 √©pocas)\")\nprint(f\"   üéµ Amostras de √°udio: {samples_added} arquivos\")\n\n# Verificar espa√ßo total usado\ntry:\n    total_recent = sum(os.path.getsize(f) for f in recent_checkpoints) / 1024 / 1024 if recent_checkpoints else 0\n    total_backup = sum(os.path.getsize(f) for f in backup_checkpoints) / 1024 / 1024 if backup_checkpoints else 0\n    print(f\"\\nüíæ Espa√ßo em disco utilizado:\")\n    print(f\"   üì¶ Checkpoints recentes: {total_recent:.1f}MB\")\n    print(f\"   üîí Checkpoints backup: {total_backup:.1f}MB\")\n    print(f\"   üìä Total checkpoints: {total_recent + total_backup:.1f}MB\")\nexcept:\n    print(\"\\n‚ö†Ô∏è N√£o foi poss√≠vel calcular espa√ßo em disco\")"
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}