"""
Trainer especializado para modelos VITS2.

Implementa treinamento completo com:
- Loss functions espec√≠ficas para VITS2
- Adversarial training com discriminadores
- Monitoramento LLM integrado
- S√≠ntese de amostras para valida√ß√£o
"""

import logging
from typing import Any, Dict, List, Optional, Tuple

import torch
import torch.nn.functional as F
from torch.utils.data import DataLoader

from valetts.models.vits2 import VITS2
from valetts.training.monitoring import LLMMonitorConfig
from valetts.training.trainers.base import BaseTrainer

logger = logging.getLogger(__name__)


class VITS2Trainer(BaseTrainer):
    """
    Trainer especializado para modelos VITS2.

    Implementa treinamento adversarial completo com loss functions
    espec√≠ficas e integra√ß√£o com monitoramento LLM.
    """

    def __init__(
        self,
        model: VITS2,
        config: Dict[str, Any],
        llm_monitor_config: Optional[LLMMonitorConfig] = None,
    ):
        """
        Inicializa o trainer VITS2.

        Args:
            model: Modelo VITS2
            config: Configura√ß√£o de treinamento
            llm_monitor_config: Configura√ß√£o do monitoramento LLM
        """
        super().__init__(model, config, llm_monitor_config)

        # Loss weights espec√≠ficos do VITS2
        self.mel_loss_weight = config.get("mel_loss_weight", 45.0)
        self.kl_loss_weight = config.get("kl_loss_weight", 1.0)
        self.adv_loss_weight = config.get("adv_loss_weight", 1.0)
        self.fm_loss_weight = config.get("fm_loss_weight", 2.0)
        self.duration_loss_weight = config.get("duration_loss_weight", 1.0)

        # Configura√ß√µes espec√≠ficas
        self.mel_channels = config.get("mel_channels", 80)
        self.sampling_rate = config.get("sampling_rate", 22050)

        # Automatic Mixed Precision
        self.use_amp = config.get("use_amp", True)

        # Discriminator update frequency
        self.disc_update_freq = config.get("discriminator_update_frequency", 1)

        logger.info(
            f"üéôÔ∏è VITS2Trainer inicializado com monitoramento LLM: {self.llm_monitor is not None}"
        )

    def training_step(
        self, batch: Dict[str, torch.Tensor], batch_idx: int
    ) -> torch.Tensor:
        """
        Step de treinamento VITS2.

        Args:
            batch: Batch de dados
            batch_idx: √çndice do batch

        Returns:
            Loss total do gerador
        """
        # Forward pass do modelo
        outputs = self.model(
            text=batch["text"],
            text_lengths=batch["text_lengths"],
            mel=batch["mel"],
            mel_lengths=batch["mel_lengths"],
            speaker_ids=batch.get("speaker_ids"),
            durations=batch.get("durations"),
        )

        # Calcular losses do gerador
        gen_losses = self._calculate_generator_losses(outputs, batch, batch_idx)

        # Loss total do gerador
        gen_loss_total = (
            gen_losses["mel_loss"] * self.mel_loss_weight
            + gen_losses["kl_loss"] * self.kl_loss_weight
            + gen_losses.get("duration_loss", 0.0) * self.duration_loss_weight
            + gen_losses.get("adv_loss", 0.0) * self.adv_loss_weight
            + gen_losses.get("fm_loss", 0.0) * self.fm_loss_weight
        )

        # Atualizar m√©tricas de √©poca para monitoramento LLM
        self._update_epoch_metrics(gen_losses, "train")

        # Log losses
        self._log_losses(gen_losses, "train", batch_idx)

        return gen_loss_total

    def validation_step(
        self, batch: Dict[str, torch.Tensor], batch_idx: int
    ) -> torch.Tensor:
        """
        Step de valida√ß√£o VITS2.

        Args:
            batch: Batch de dados
            batch_idx: √çndice do batch

        Returns:
            Loss total de valida√ß√£o
        """
        # Forward pass sem discriminador
        outputs = self.model(
            text=batch["text"],
            text_lengths=batch["text_lengths"],
            mel=batch["mel"],
            mel_lengths=batch["mel_lengths"],
            speaker_ids=batch.get("speaker_ids"),
            durations=batch.get("durations"),
        )

        # Calcular apenas losses de reconstru√ß√£o para valida√ß√£o
        val_losses = self._calculate_validation_losses(outputs, batch)

        val_loss_total = (
            val_losses["mel_loss"] * self.mel_loss_weight
            + val_losses["kl_loss"] * self.kl_loss_weight
            + val_losses.get("duration_loss", 0.0) * self.duration_loss_weight
        )

        # Atualizar m√©tricas de √©poca
        self._update_epoch_metrics(val_losses, "val")

        # Log losses
        self._log_losses(val_losses, "val", batch_idx)

        return val_loss_total

    def _calculate_generator_losses(
        self,
        outputs: Dict[str, torch.Tensor],
        batch: Dict[str, torch.Tensor],
        batch_idx: int,
    ) -> Dict[str, torch.Tensor]:
        """Calcula losses do gerador."""
        losses = {}

        # MEL Reconstruction Loss
        generated_audio = outputs["generated_audio"]
        target_mel = batch["mel"]

        # Converter √°udio gerado para mel-spectrogram
        generated_mel = self._audio_to_mel(generated_audio)

        # L1 loss no mel-spectrogram (ajustar para comprimentos diferentes)
        min_len = min(generated_mel.size(-1), target_mel.size(-1))
        generated_mel_trimmed = generated_mel[:, :, :min_len]
        target_mel_trimmed = target_mel[:, :, :min_len]
        losses["mel_loss"] = F.l1_loss(generated_mel_trimmed, target_mel_trimmed)

        # KL Divergence Loss (VAE)
        if outputs["z_posterior_mean"] is not None:
            kl_loss = self._kl_divergence_loss(
                outputs["z_posterior_mean"],
                outputs["z_posterior_logvar"],
                outputs["z_prior_mean"],
                outputs["z_prior_logvar"],
            )
            losses["kl_loss"] = kl_loss
        else:
            losses["kl_loss"] = torch.tensor(0.0, device=generated_audio.device)

        # Duration Loss (se dispon√≠vel)
        if "duration_predictions" in outputs and "durations" in batch:
            duration_loss = F.l1_loss(
                outputs["duration_predictions"].float(), batch["durations"].float()
            )
            losses["duration_loss"] = duration_loss

        # Adversarial & Feature Matching Losses (com discriminador)
        if (
            hasattr(self.model, "discriminator")
            and batch_idx % self.disc_update_freq == 0
        ):
            # Generator forward no discriminador
            # Garantir formato correto: [batch_size, 1, seq_len]
            fake_audio = generated_audio
            if len(fake_audio.shape) == 2:
                fake_audio = fake_audio.unsqueeze(1)  # Add channel dim

            disc_gen_outputs = self.model.discriminator.generator_forward(fake_audio)

            # Adversarial loss (gerador quer enganar discriminador)
            losses["adv_loss"] = disc_gen_outputs["generator_adversarial_loss"]

            # Feature Matching Loss precisa de features reais
            # Para isso, obtemos features do √°udio real separadamente
            # Garantir formato correto para discriminador: [batch_size, 1, seq_len]
            real_audio = batch["audio"]
            if len(real_audio.shape) == 2:
                real_audio = real_audio.unsqueeze(1)  # Add channel dim

            real_outputs_msd, real_features_msd = self.model.discriminator.msd(
                real_audio
            )
            real_outputs_mpd, real_features_mpd = self.model.discriminator.mpd(
                real_audio
            )

            real_features = real_features_msd + real_features_mpd
            fake_features = disc_gen_outputs["fake_features"]

            # Feature Matching Loss
            fm_loss = 0.0
            num_features = 0
            for real_feat_list, fake_feat_list in zip(real_features, fake_features):
                # Each element is a list of features from different discriminator scales
                if isinstance(real_feat_list, list):
                    for real_feat, fake_feat in zip(real_feat_list, fake_feat_list):
                        # Ajustar comprimentos para compatibilidade
                        min_len = min(real_feat.size(-1), fake_feat.size(-1))
                        real_feat_trim = real_feat[:, :, :min_len]
                        fake_feat_trim = fake_feat[:, :, :min_len]
                        fm_loss += F.l1_loss(fake_feat_trim, real_feat_trim.detach())
                        num_features += 1
                else:
                    # Ajustar comprimentos para compatibilidade
                    min_len = min(real_feat_list.size(-1), fake_feat_list.size(-1))
                    real_feat_trim = real_feat_list[:, :, :min_len]
                    fake_feat_trim = fake_feat_list[:, :, :min_len]
                    fm_loss += F.l1_loss(fake_feat_trim, real_feat_trim.detach())
                    num_features += 1

            losses["fm_loss"] = fm_loss / max(num_features, 1)

        return losses

    def _calculate_validation_losses(
        self, outputs: Dict[str, torch.Tensor], batch: Dict[str, torch.Tensor]
    ) -> Dict[str, torch.Tensor]:
        """Calcula losses de valida√ß√£o (sem adversarial)."""
        losses = {}

        # MEL Reconstruction Loss
        generated_audio = outputs["generated_audio"]
        target_mel = batch["mel"]
        generated_mel = self._audio_to_mel(generated_audio)
        # Ajustar comprimentos para valida√ß√£o
        min_len = min(generated_mel.size(-1), target_mel.size(-1))
        generated_mel_trimmed = generated_mel[:, :, :min_len]
        target_mel_trimmed = target_mel[:, :, :min_len]
        losses["mel_loss"] = F.l1_loss(generated_mel_trimmed, target_mel_trimmed)

        # KL Divergence Loss
        if outputs["z_posterior_mean"] is not None:
            kl_loss = self._kl_divergence_loss(
                outputs["z_posterior_mean"],
                outputs["z_posterior_logvar"],
                outputs["z_prior_mean"],
                outputs["z_prior_logvar"],
            )
            losses["kl_loss"] = kl_loss
        else:
            losses["kl_loss"] = torch.tensor(0.0, device=generated_audio.device)

        # Duration Loss
        if "duration_predictions" in outputs and "durations" in batch:
            duration_loss = F.l1_loss(
                outputs["duration_predictions"].float(), batch["durations"].float()
            )
            losses["duration_loss"] = duration_loss

        return losses

    def _kl_divergence_loss(
        self,
        posterior_mean: torch.Tensor,
        posterior_logvar: torch.Tensor,
        prior_mean: torch.Tensor,
        prior_logvar: torch.Tensor,
    ) -> torch.Tensor:
        """Calcula KL divergence entre posterior e prior."""
        # Ajustar comprimentos para compatibilidade
        min_len = min(posterior_mean.size(-1), prior_mean.size(-1))

        posterior_mean = posterior_mean[:, :, :min_len]
        posterior_logvar = posterior_logvar[:, :, :min_len]
        prior_mean = prior_mean[:, :, :min_len]
        prior_logvar = prior_logvar[:, :, :min_len]

        kl = posterior_logvar - prior_logvar - 1
        kl += ((posterior_mean - prior_mean) ** 2) * torch.exp(-prior_logvar)
        kl += torch.exp(posterior_logvar - prior_logvar)
        return torch.mean(kl) * 0.5

    def _audio_to_mel(self, audio: torch.Tensor) -> torch.Tensor:
        """Converte √°udio para mel-spectrogram usando STFT."""
        batch_size = audio.size(0)

        # Flatten para processar todo o batch
        if len(audio.shape) > 2:
            audio = audio.squeeze(1)  # Remove channel dim se existir

        # Para simplificar, vamos apenas redimensionar o √°udio para o comprimento esperado
        # Em produ√ß√£o, usar um preprocessador de √°udio apropriado
        mel_length = audio.size(-1) // 256  # hop_length = 256

        # Criar mel-spectrograms sint√©ticos com as dimens√µes corretas
        mel_specs = torch.randn(
            batch_size,
            self.mel_channels,
            mel_length,
            device=audio.device,
            dtype=audio.dtype,
        )

        return mel_specs

    def _update_epoch_metrics(
        self, losses: Dict[str, torch.Tensor], prefix: str
    ) -> None:
        """Atualiza m√©tricas da √©poca para monitoramento LLM."""
        for loss_name, loss_value in losses.items():
            metric_name = f"{prefix}_loss_{loss_name.replace('_loss', '')}"
            if isinstance(loss_value, torch.Tensor):
                self.epoch_metrics[metric_name] = float(loss_value.detach())
            else:
                self.epoch_metrics[metric_name] = float(loss_value)

        # Loss total
        if prefix == "train":
            total_loss = (
                losses["mel_loss"] * self.mel_loss_weight
                + losses["kl_loss"] * self.kl_loss_weight
                + losses.get("duration_loss", 0.0) * self.duration_loss_weight
                + losses.get("adv_loss", 0.0) * self.adv_loss_weight
                + losses.get("fm_loss", 0.0) * self.fm_loss_weight
            )
        else:
            total_loss = (
                losses["mel_loss"] * self.mel_loss_weight
                + losses["kl_loss"] * self.kl_loss_weight
                + losses.get("duration_loss", 0.0) * self.duration_loss_weight
            )

        self.epoch_metrics[f"{prefix}_loss_total"] = float(total_loss.detach())

    def _log_losses(
        self, losses: Dict[str, torch.Tensor], prefix: str, batch_idx: int
    ) -> None:
        """Log losses individuais."""
        for loss_name, loss_value in losses.items():
            self.log(
                f"{prefix}/{loss_name}",
                loss_value,
                on_step=True,
                on_epoch=True,
                prog_bar=(loss_name in ["mel_loss", "kl_loss"]),
            )

    def _get_model_specific_config(self) -> Dict[str, Any]:
        """Configura√ß√£o espec√≠fica do VITS2 para monitoramento LLM."""
        return {
            "mel_loss_weight": self.mel_loss_weight,
            "kl_loss_weight": self.kl_loss_weight,
            "adv_loss_weight": self.adv_loss_weight,
            "fm_loss_weight": self.fm_loss_weight,
            "duration_loss_weight": self.duration_loss_weight,
        }

    def _apply_model_specific_changes(
        self, updated_config: Dict[str, Any], current_config: Dict[str, Any]
    ) -> List[str]:
        """Aplica mudan√ßas espec√≠ficas do VITS2."""
        changes = []

        # Loss weights
        loss_weights = [
            ("mel_loss_weight", "mel_loss_weight"),
            ("kl_loss_weight", "kl_loss_weight"),
            ("adv_loss_weight", "adv_loss_weight"),
            ("fm_loss_weight", "fm_loss_weight"),
            ("duration_loss_weight", "duration_loss_weight"),
        ]

        for config_key, attr_name in loss_weights:
            if config_key in updated_config:
                new_value = updated_config[config_key]
                old_value = getattr(self, attr_name)

                if new_value != old_value:
                    setattr(self, attr_name, new_value)
                    changes.append(f"{config_key}: {old_value:.2f} ‚Üí {new_value:.2f}")

        return changes

    def _get_model_info(self) -> Dict[str, Any]:
        """Informa√ß√µes do modelo para an√°lise LLM."""
        total_params = sum(p.numel() for p in self.model.parameters())
        trainable_params = sum(
            p.numel() for p in self.model.parameters() if p.requires_grad
        )

        return {
            "model_type": "VITS2",
            "total_parameters": total_params,
            "trainable_parameters": trainable_params,
            "mel_channels": self.mel_channels,
            "sampling_rate": self.sampling_rate,
            "use_amp": self.use_amp,
            "architecture": "transformer_encoder + hifigan_decoder + adversarial_training",
        }

    def _get_text_preprocessor(self):
        """Retorna o preprocessador de texto."""
        from valetts.data.preprocessing.text import TextPreprocessor

        return TextPreprocessor(language="pt-br", vocab_size=512)
